import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import json
import functools
import inspect
import sys
import argparse
import dataclasses
import random  # For jitter
import aiohttp  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º aiohttp
import base64  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º base64
import time
import binascii

from enum import Enum
from urllib.parse import urlparse, parse_qs
from typing import Dict, List, Optional, Tuple, Set, DefaultDict # <-- –£—Ç–æ—á–Ω–∏–ª DefaultDict
from dataclasses import dataclass, field
from collections import defaultdict
from string import Template  # For flexible profile names

# --- Constants ---
LOG_FILE = 'proxy_downloader.log'
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"
LOG_FORMAT = {
    "time": "%(asctime)s",
    "level": "%(levelname)s",
    "message": "%(message)s",
    "process": "%(process)s",
    "module": "%(module)s",
    "funcName": "%(funcName)s",
    "lineno": "%(lineno)d",
}

DNS_TIMEOUT = 15  # seconds
HTTP_TIMEOUT = 15  # seconds
MAX_RETRIES = 4
RETRY_DELAY_BASE = 2
HEADERS = {'User-Agent': 'ProxyDownloader/1.0'}
PROTOCOL_REGEX = re.compile(r"^(vless|tuic|hy2|ss|ssr|trojan)://", re.IGNORECASE)
# –ü—Ä–æ—Å—Ç–æ–π regex –¥–ª—è –±–∞–∑–æ–≤–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ hostname (–¥–æ–ø—É—Å–∫–∞–µ—Ç –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –¥–µ—Ñ–∏—Å—ã, —Ç–æ—á–∫–∏)
HOSTNAME_REGEX = re.compile(r"^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$")
PROFILE_NAME_TEMPLATE = Template("${protocol}-${type}-${security}")  # Concise profile names

COLOR_MAP = {
    logging.INFO: '\033[92m',    # GREEN
    logging.WARNING: '\033[93m', # YELLOW
    logging.ERROR: '\033[91m',   # RED
    logging.CRITICAL: '\033[1m\033[91m', # BOLD_RED
    'RESET': '\033[0m'
}

QUALITY_SCORE_WEIGHTS = {
    "protocol": {"vless": 5, "trojan": 5, "tuic": 4, "hy2": 3, "ss": 2, "ssr": 1},
    "security": {"tls": 3, "none": 0},
    "transport": {"ws": 2, "websocket": 2, "grpc": 2, "tcp": 1, "udp": 0},
}

QUALITY_CATEGORIES = {
    "High": range(8, 15),
    "Medium": range(4, 8),
    "Low": range(0, 4),
}

# --- Data Structures ---
class Protocols(Enum):
    """Enumeration of supported proxy protocols."""
    VLESS = "vless"
    TUIC = "tuic"
    HY2 = "hy2"
    SS = "ss"
    SSR = "ssr"
    TROJAN = "trojan"

ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]


# --- Logging Setup ---
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)

class JsonFormatter(logging.Formatter):
    """Formatter for JSON log output."""
    def format(self, record):
        log_record = LOG_FORMAT.copy()
        log_record["message"] = record.getMessage()
        log_record["level"] = record.levelname
        log_record["process"] = record.process
        log_record["time"] = self.formatTime(record, self.default_time_format)
        log_record["module"] = record.module
        log_record["funcName"] = record.funcName
        log_record["lineno"] = record.lineno
        if record.exc_info:
            log_record['exc_info'] = self.formatException(record.exc_info)
        return json.dumps(log_record, ensure_ascii=False)

formatter_file = JsonFormatter()
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

class ColoredFormatter(logging.Formatter):
    """Formatter for colored console output."""
    def __init__(self, fmt=CONSOLE_LOG_FORMAT, use_colors=True):
        super().__init__(fmt)
        self.use_colors = use_colors

    def format(self, record):
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º, –ø–æ—Ç–æ–º –æ–∫—Ä–∞—à–∏–≤–∞–µ–º
        message = super().format(record)
        if self.use_colors:
            color_start = COLOR_MAP.get(record.levelno, COLOR_MAP['RESET'])
            color_reset = COLOR_MAP['RESET']
            message = f"{color_start}{message}{color_reset}"
        return message

console_formatter = ColoredFormatter()
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)


def colored_log(level: int, message: str, *args, **kwargs):
    """Logs a message with color to the console using standard logging."""
    # –û—Å—Ç–∞–≤–ª—è–µ–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é, —Ç.–∫. –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Å—Ç–∞—Ö –¥–ª—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
    logger.log(level, message, *args, **kwargs)


# --- Data Structures ---
@dataclass(frozen=True)
class ConfigFiles:
    """Configuration file paths."""
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt"

@dataclass(frozen=True)
class RetrySettings:
    """Settings for retry attempts."""
    MAX_RETRIES: int = MAX_RETRIES
    RETRY_DELAY_BASE: int = RETRY_DELAY_BASE

@dataclass(frozen=True)
class ConcurrencyLimits:
    """Limits for concurrency."""
    MAX_CHANNELS: int = 60
    MAX_PROXIES_PER_CHANNEL: int = 50 # –≠—Ç–æ—Ç –ª–∏–º–∏—Ç —Ç–µ–ø–µ—Ä—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ –∫–æ–¥–µ
    MAX_PROXIES_GLOBAL: int = 50 # –≠—Ç–æ—Ç –ª–∏–º–∏—Ç —Ç–µ–ø–µ—Ä—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ –∫–æ–¥–µ

CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()


class ProfileName(Enum):
    """Enumeration for proxy profile names."""
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    SSR = "SSR"
    TROJAN = "TROJAN"
    UNKNOWN = "Unknown Protocol"

# --- Custom Exceptions ---
class InvalidURLError(ValueError):
    """Exception for invalid URLs."""
    pass

class UnsupportedProtocolError(ValueError):
    """Exception for unsupported protocols."""
    pass

class EmptyChannelError(Exception): # <-- –ù–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
    """Exception raised when a channel returns an empty response."""
    pass

class DownloadError(Exception): # <-- –ù–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
    """General exception for download failures (retries exhausted, critical errors)."""
    pass


@dataclass(frozen=True, eq=True)
class ProxyParsedConfig:
    """Represents a parsed proxy configuration."""
    config_string: str
    protocol: str
    address: str
    port: int
    remark: str = ""
    query_params: Dict[str, str] = field(default_factory=dict)
    quality_score: int = 0 # –ë—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ –≤ resolve_and_assess_proxies

    def __hash__(self):
        """Hashes the configuration string for deduplication."""
        # –•–µ—à–∏—Ä—É–µ–º –∏–º–µ–Ω–Ω–æ config_string, —Ç.–∫. –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        # –¥–æ —ç—Ç–∞–ø–∞ —Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞ –∏ –æ—Ü–µ–Ω–∫–∏
        return hash(self.config_string)

    def __str__(self):
        """String representation of the ProxyConfig object."""
        # –£–±—Ä–∞–ª–∏ quality_score –æ—Ç—Å—é–¥–∞, —Ç.–∫. –æ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø–æ–∑–∂–µ
        return (f"ProxyParsedConfig(protocol={self.protocol}, address={self.address}, "
                f"port={self.port}, config_string='{self.config_string[:50]}...')")

    @staticmethod
    def _decode_base64_if_needed(config_string: str) -> Tuple[str, bool]:
        """
        Decodes base64 if the string doesn't start with a known protocol.
        Applies padding and specific error handling.
        """
        if PROTOCOL_REGEX.match(config_string):
            return config_string, False
        try:
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –º–µ—à–∞—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é
            possible_base64 = "".join(config_string.split())
            # –î–æ–±–∞–≤–ª—è–µ–º padding, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
            missing_padding = len(possible_base64) % 4
            if missing_padding:
                possible_base64 += '=' * (4 - missing_padding)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º validate=True –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ Base64
            decoded_bytes = base64.b64decode(possible_base64, validate=True)
            decoded_config = decoded_bytes.decode('utf-8')

            if PROTOCOL_REGEX.match(decoded_config):
                return decoded_config, True
            else:
                # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–ª–æ—Å—å, –Ω–æ –Ω–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª
                logger.debug(f"Decoded string doesn't match known protocols: {decoded_config[:50]}...")
                return config_string, False
        except (binascii.Error, UnicodeDecodeError) as e: # <-- –õ–æ–≤–∏–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏
            # –ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ Base64 –∏–ª–∏ UTF-8
            logger.debug(f"Base64/UTF-8 decoding failed for '{config_string[:50]}...': {e}")
            return config_string, False
        except Exception as e:
             # –õ–æ–≤–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –∏—Ö –æ—Ç–¥–µ–ª—å–Ω–æ
             logger.error(f"Unexpected error decoding base64 for '{config_string[:50]}...': {e}", exc_info=True)
             return config_string, False

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        """
        Parses a proxy configuration URL, performs basic validation,
        and handles query parameters safely.
        """
        original_string_for_hash = config_string.strip() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Ö–µ—à–∞
        config_string, was_decoded = cls._decode_base64_if_needed(original_string_for_hash)

        protocol_match = PROTOCOL_REGEX.match(config_string)
        if not protocol_match:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤ _decode_base64_if_needed –∏–ª–∏ —ç—Ç–æ –Ω–µ URL
            # logger.debug(f"Not a valid proxy URL format: {config_string[:100]}...")
            return None
        protocol = protocol_match.group(1).lower()

        try:
            parsed_url = urlparse(config_string)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ö–µ–º—ã (—Ö–æ—Ç—è PROTOCOL_REGEX —É–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª –Ω–∞—á–∞–ª–æ)
            if parsed_url.scheme.lower() != protocol:
                logger.debug(f"URL scheme '{parsed_url.scheme}' mismatch for protocol '{protocol}': {config_string}")
                return None

            address = parsed_url.hostname
            port = parsed_url.port

            # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∞–¥—Ä–µ—Å–∞/–ø–æ—Ä—Ç–∞
            if not address or not port:
                logger.debug(f"Address or port missing in URL: {config_string}")
                return None

            # <-- –î–æ–±–∞–≤–ª–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è hostname
            if not is_valid_ipv4(address) and not HOSTNAME_REGEX.match(address):
                 logger.debug(f"Invalid hostname format: {address} in URL: {config_string}")
                 return None

            if not 1 <= port <= 65535:
                logger.debug(f"Invalid port number: {port} in URL: {config_string}")
                return None

            remark = parsed_url.fragment or ""
            # <-- –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ query parameters (–±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
            query_params_raw = parse_qs(parsed_url.query)
            query_params = {k: v[0] for k, v in query_params_raw.items() if v} # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å–ø–∏—Å–æ–∫ v –Ω–µ –ø—É—Å—Ç–æ–π

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º original_string_for_hash –∏–ª–∏ config_string –±–µ–∑ fragment –¥–ª—è config_string?
            # –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –±–µ–∑ fragment –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            config_string_to_store = config_string.split('#')[0]

            return cls(
                config_string=config_string_to_store, # –°–æ—Ö—Ä–∞–Ω—è–µ–º URL –±–µ–∑ fragment
                protocol=protocol,
                address=address,
                port=port,
                remark=remark,
                query_params=query_params,
                # quality_score –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ
            )

        except ValueError as e:
            logger.debug(f"URL parsing error for '{config_string[:100]}...': {e}")
            return None


# --- Helper Functions ---
@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """Checks if a string is a valid IPv4 address."""
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """Resolves a hostname to an IPv4 address with timeout and error handling."""
    if is_valid_ipv4(hostname):
        return hostname

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.timeout –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞
        async with asyncio.timeout(DNS_TIMEOUT):
            result = await resolver.query(hostname, 'A')
            if result:
                resolved_ip = result[0].host
                if is_valid_ipv4(resolved_ip):
                    logger.debug(f"DNS resolved {hostname} to {resolved_ip}")
                    return resolved_ip
                else:
                    logger.debug(f"DNS resolved {hostname} to non-IPv4: {resolved_ip}")
                    return None
            else:
                 logger.debug(f"DNS query for {hostname} returned no results.")
                 return None
    except asyncio.TimeoutError:
        logger.debug(f"DNS resolution timeout for {hostname}")
        return None
    except aiodns.error.DNSError as e:
        error_code = e.args[0] if e.args else "Unknown"
        # –£—Ç–æ—á–Ω—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
        if error_code == 4: # NXDOMAIN
             logger.debug(f"DNS resolution error for {hostname}: Host not found (NXDOMAIN)")
        elif error_code == 1: # FORMERR
             logger.debug(f"DNS resolution error for {hostname}: Format error (FORMERR)")
        else:
             logger.debug(f"DNS resolution error for {hostname}: {e}, Code: {error_code}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error during DNS resolution for {hostname}: {e}", exc_info=True)
        return None

def assess_proxy_quality(proxy_config: ProxyParsedConfig) -> int:
    """Assesses proxy quality based on configuration using weights."""
    score = 0
    protocol = proxy_config.protocol.lower()
    query_params = proxy_config.query_params

    score += QUALITY_SCORE_WEIGHTS["protocol"].get(protocol, 0)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'security' –∏–∑ query_params, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ 'none'
    security = query_params.get("security", "none").lower()
    score += QUALITY_SCORE_WEIGHTS["security"].get(security, 0)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'type' –∏–ª–∏ 'transport' –∏–∑ query_params, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ 'tcp'
    transport = query_params.get("type", query_params.get("transport", "tcp")).lower()
    score += QUALITY_SCORE_WEIGHTS["transport"].get(transport, 0)

    return score

def get_quality_category(score: int) -> str:
    """Determines quality category based on the score."""
    for category, score_range in QUALITY_CATEGORIES.items():
        if score in score_range:
            return category
    return "Unknown" # –ò–ª–∏ "Low" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é?

def generate_proxy_profile_name(proxy_config: ProxyParsedConfig) -> str:
    """Generates a concise proxy profile name using a template."""
    protocol = proxy_config.protocol.upper()
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'type' –∏–ª–∏ 'transport' –¥–ª—è –∏–º–µ–Ω–∏, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ 'type'
    type_ = proxy_config.query_params.get('type', proxy_config.query_params.get('transport', 'tcp')).lower()
    security = proxy_config.query_params.get('security', 'none').lower()

    profile_name_values = {
        "protocol": protocol,
        "type": type_,
        "security": security
    }
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–ª—é—á–∞
    return PROFILE_NAME_TEMPLATE.safe_substitute(profile_name_values)


# --- Core Logic Functions ---

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> List[str]:
    """
    Downloads proxy configurations from a channel URL with retry logic.
    Handles Base64 decoding and returns a list of lines or raises exceptions.
    """
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=HTTP_TIMEOUT)

    while retries_attempted <= RETRY.MAX_RETRIES:
        try:
            logger.debug(f"Attempting download from {channel_url} (Attempt {retries_attempted + 1})")
            async with session.get(channel_url, timeout=session_timeout, headers=HEADERS) as response:
                # –í—ã–±—Ä–æ—Å–∏—Ç ClientResponseError –ø—Ä–∏ 4xx/5xx
                response.raise_for_status()
                logger.debug(f"Successfully connected to {channel_url}, status: {response.status}")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º read(), —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–∞–π—Ç—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –Ω–∞–¥–µ–∂–Ω–µ–µ
                content_bytes = await response.read()
                if not content_bytes.strip():
                    logger.warning(f"Channel {channel_url} returned empty response.")
                    raise EmptyChannelError(f"Channel {channel_url} returned empty response.")

                # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É (aiohttp –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–ª—è text(), –Ω–æ –º–æ–∂–Ω–æ –∏ –≤—Ä—É—á–Ω—É—é)
                text: str
                try:
                    text = content_bytes.decode('utf-8')
                    logger.debug(f"Decoded content from {channel_url} as UTF-8")
                except UnicodeDecodeError:
                    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å replace/ignore
                    logger.warning(f"UTF-8 decoding failed for {channel_url}, replacing errors.")
                    text = content_bytes.decode('utf-8', errors='replace') # –ò—Å–ø–æ–ª—å–∑—É–µ–º replace

                # –ü–æ–ø—ã—Ç–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è Base64 (–ª–æ–≥–∏–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ _decode_base64_if_needed)
                try:
                    possible_base64 = "".join(text.strip().split())
                    missing_padding = len(possible_base64) % 4
                    if missing_padding:
                        possible_base64 += '=' * (4 - missing_padding)

                    decoded_bytes = base64.b64decode(possible_base64, validate=True)
                    decoded_text = decoded_bytes.decode('utf-8')
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–æ–∫—Å–∏-—Å—Å—ã–ª–∫–∏
                    if PROTOCOL_REGEX.search(decoded_text): # –ò—â–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ, –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –Ω–∞—á–∞–ª–µ
                        logger.debug(f"Content from {channel_url} successfully decoded as Base64.")
                        return decoded_text.splitlines()
                    else:
                        logger.debug(f"Content from {channel_url} decoded from Base64, but no protocol found. Using original text.")
                        return text.splitlines()
                except (binascii.Error, UnicodeDecodeError):
                    # –ù–µ Base64 –∏–ª–∏ –æ—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ Base64 -> –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    logger.debug(f"Content from {channel_url} is not valid Base64 or UTF-8 after decode. Using as plain text.")
                    return text.splitlines()

        except aiohttp.ClientResponseError as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            colored_log(logging.WARNING, f"‚ö†Ô∏è Channel {channel_url} returned HTTP error {e.status}: {e.message}")
            logger.debug(f"Response headers for {channel_url} on error: {response.headers}")
            # –ù–µ —Ä–µ—Ç—Ä–∞–∏–º –Ω–∞ –æ—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞/—Å–µ—Ä–≤–µ—Ä–∞ (4xx/5xx) - –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–∞–∑—É
            raise DownloadError(f"HTTP error {e.status} for {channel_url}") from e
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            # –û—à–∏–±–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, —Ç–∞–π–º–∞—É—Ç—ã - —Ä–µ—Ç—Ä–∞–∏–º
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted) + random.uniform(-0.5, 0.5)
            retry_delay = max(0.5, retry_delay) # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            colored_log(logging.WARNING, f"‚ö†Ô∏è Error getting {channel_url} (attempt {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {type(e).__name__}. Retry in {retry_delay:.2f}s...")
            if retries_attempted == RETRY.MAX_RETRIES:
                colored_log(logging.ERROR, f"‚ùå Max retries ({RETRY.MAX_RETRIES+1}) reached for {channel_url}")
                raise DownloadError(f"Max retries reached for {channel_url}") from e
            await asyncio.sleep(retry_delay)
        except EmptyChannelError as e: # –õ–æ–≤–∏–º –Ω–∞—à–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
             # –ù–µ —Ä–µ—Ç—Ä–∞–∏–º –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
             raise e # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞–ª—å—à–µ
        except Exception as e:
             # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ - –Ω–µ —Ä–µ—Ç—Ä–∞–∏–º, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
             logger.error(f"Unexpected error downloading {channel_url}: {e}", exc_info=True)
             raise DownloadError(f"Unexpected error downloading {channel_url}") from e

        retries_attempted += 1

    # –ï—Å–ª–∏ —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à–∏–ª—Å—è (–Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–µ)
    logger.critical(f"Download loop finished unexpectedly for {channel_url}")
    raise DownloadError(f"Download failed unexpectedly after retries for {channel_url}")


# --- –†–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞ ---

def parse_proxy_lines(lines: List[str]) -> Tuple[List[ProxyParsedConfig], int, int]:
    """
    Parses lines into ProxyParsedConfig objects, performs basic validation,
    and initial deduplication based on the config string.
    Returns: List of parsed configs, count of invalid urls, count of duplicates.
    """
    parsed_configs: List[ProxyParsedConfig] = []
    processed_strings: Set[str] = set() # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–æ–∫–µ –∫–æ–Ω—Ñ–∏–≥–∞
    invalid_url_count = 0
    duplicate_count = 0

    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):
            continue

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ProxyParsedConfig.from_url –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –±–∞–∑–æ–≤–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        parsed_config = ProxyParsedConfig.from_url(line)

        if parsed_config is None:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ from_url –∏–ª–∏ _decode_base64
            invalid_url_count += 1
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –î–û —Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º config_string, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –æ–±—ä–µ–∫—Ç–µ
        if parsed_config.config_string in processed_strings:
            logger.debug(f"Skipping duplicate proxy (based on string): {parsed_config.config_string[:50]}...")
            duplicate_count += 1
            continue
        processed_strings.add(parsed_config.config_string)

        parsed_configs.append(parsed_config)

    logger.info(f"Initial parsing: {len(parsed_configs)} potentially valid configs found. "
                f"Skipped {invalid_url_count} invalid lines, {duplicate_count} duplicates (string-based).")
    return parsed_configs, invalid_url_count, duplicate_count


async def resolve_and_assess_proxies(
    configs: List[ProxyParsedConfig], resolver: aiodns.DNSResolver
) -> Tuple[List[ProxyParsedConfig], int]:
    """
    Resolves DNS addresses and assesses quality for a list of parsed configs.
    Returns: List of resolved and assessed configs, count of DNS resolution failures.
    """
    resolved_configs_with_score: List[ProxyParsedConfig] = []
    dns_resolution_failed_count = 0

    async def resolve_task(config: ProxyParsedConfig) -> Optional[ProxyParsedConfig]:
        nonlocal dns_resolution_failed_count
        resolved_ip = await resolve_address(config.address, resolver)
        if resolved_ip:
            # –ê–¥—Ä–µ—Å —Ä–∞–∑—Ä–µ—à–∏–ª—Å—è, –æ—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            quality_score = assess_proxy_quality(config)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º score
            # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å resolved_ip –≤ –æ–±—ä–µ–∫—Ç, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            return dataclasses.replace(config, quality_score=quality_score)
        else:
            # DNS resolution failed
            logger.debug(f"DNS resolution failed for proxy address: {config.address} from config: {config.config_string[:50]}...")
            dns_resolution_failed_count += 1
            return None

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ —Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = [resolve_task(cfg) for cfg in configs]
    results = await asyncio.gather(*tasks)

    # –°–æ–±–∏—Ä–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    resolved_configs_with_score = [res for res in results if res is not None]

    logger.info(f"DNS Resolution & Assessment: {len(resolved_configs_with_score)} configs resolved and assessed. "
                f"{dns_resolution_failed_count} DNS resolution failures.")
    return resolved_configs_with_score, dns_resolution_failed_count


# --- –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∏ —É–ø—Ä–æ—â–µ–Ω–∞) ---

def save_unique_proxies_to_file(unique_proxies: List[ProxyParsedConfig], output_file: str) -> int:
    """
    Saves a list of unique, assessed proxies to a file.
    Assumes the input list `unique_proxies` is already deduplicated.
    Returns the number of proxies successfully written.
    """
    count = 0
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        logger.info(f"Attempting to save {len(unique_proxies)} unique proxies to {output_file}")

        lines_to_write = []
        for proxy_conf in unique_proxies: # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Å–ø–∏—Å–æ–∫ —É–∂–µ —É–Ω–∏–∫–∞–ª–µ–Ω
            profile_name = generate_proxy_profile_name(proxy_conf)
            quality_category = get_quality_category(proxy_conf.quality_score)
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ #
            config_line = (f"{proxy_conf.config_string}#{profile_name}_"
                           f"Q{proxy_conf.quality_score}_{quality_category}\n")
                           # –ü—Ä–∏–º–µ—Ä: vless://...@host?params#VLESS-WS-TLS_Q10_High
            lines_to_write.append(config_line)
            count += 1

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º writelines –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines_to_write)

        logger.info(f"Successfully wrote {count} proxies to {output_file}")

    except IOError as e:
        logger.error(f"IOError saving proxies to file '{output_file}': {e}", exc_info=True)
        return 0 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 0 –ø—Ä–∏ –æ—à–∏–±–∫–µ
    except Exception as e:
        logger.error(f"Unexpected error saving proxies to file '{output_file}': {e}", exc_info=True)
        return 0 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 0 –ø—Ä–∏ –æ—à–∏–±–∫–µ
    return count


async def load_channel_urls(all_urls_file: str) -> List[str]:
    """Loads channel URLs from a file, handling BOM, encoding, and comments."""
    channel_urls: List[str] = []
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º utf-8-sig –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ BOM
        with open(all_urls_file, 'r', encoding='utf-8-sig') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'):
                    channel_urls.append(url)
        logger.info(f"Loaded {len(channel_urls)} channel URLs from {all_urls_file}")
    except FileNotFoundError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è File {all_urls_file} not found. Creating an empty file.")
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            os.makedirs(os.path.dirname(all_urls_file) or '.', exist_ok=True)
            open(all_urls_file, 'w').close()
        except Exception as e:
            logger.error(f"Error creating file {all_urls_file}: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"Error opening/reading file {all_urls_file}: {e}", exc_info=True)
    return channel_urls


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ ---

async def process_channel_task(channel_url: str, session: aiohttp.ClientSession,
                              resolver: aiodns.DNSResolver
                              ) -> List[ProxyParsedConfig]: # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏
    """
    Processes a single channel: downloads, parses, resolves, and assesses proxies.
    Returns a list of valid ProxyParsedConfig objects found in the channel.
    """
    colored_log(logging.INFO, f"üöÄ Processing channel: {channel_url}")
    try:
        # –®–∞–≥ 1: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ (–º–æ–∂–µ—Ç –≤—ã–±—Ä–æ—Å–∏—Ç—å DownloadError, EmptyChannelError)
        lines = await download_proxies_from_channel(channel_url, session)

        # –®–∞–≥ 2: –ü–µ—Ä–≤–∏—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å—Ç—Ä–æ–∫
        parsed_proxies_basic, _, _ = parse_proxy_lines(lines) # –°—á–µ—Ç—á–∏–∫–∏ –ø–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–¥–µ—Å—å
        if not parsed_proxies_basic:
             logger.info(f"No potentially valid configs found after parsing {channel_url}")
             return []

        # –®–∞–≥ 3: –†–µ–∑–æ–ª–≤–∏–Ω–≥ DNS –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        resolved_proxies, _ = await resolve_and_assess_proxies(parsed_proxies_basic, resolver) # –°—á–µ—Ç—á–∏–∫ DNS –æ—à–∏–±–æ–∫ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–¥–µ—Å—å

        channel_proxies_count = len(resolved_proxies)
        colored_log(logging.INFO, f"‚úÖ Channel {channel_url} processed. Found {channel_proxies_count} valid proxies.")
        return resolved_proxies

    except EmptyChannelError:
         colored_log(logging.WARNING, f"‚ö†Ô∏è Channel {channel_url} was empty or returned no parsable content.")
         return [] # –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –Ω–æ –ø—É—Å—Ç
    except DownloadError as e:
         # –û—à–∏–±–∫–∞ —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞ –≤ download_proxies_from_channel
         colored_log(logging.ERROR, f"‚ùå Failed to process channel {channel_url} after retries: {e}")
         return [] # –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞
    except Exception as e:
         # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ/—Ä–µ–∑–æ–ª–≤–∏–Ω–≥–µ
         logger.error(f"Unexpected error processing channel {channel_url}: {e}", exc_info=True)
         return [] # –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤ ---

async def load_and_process_channels(channel_urls: List[str], session: aiohttp.ClientSession,
                                     resolver: aiodns.DNSResolver
                                     ) -> Tuple[int, int, DefaultDict[str, int], List[ProxyParsedConfig], DefaultDict[str, int], DefaultDict[str, int]]:
    """
    Loads and processes all channel URLs concurrently, performs final deduplication,
    and aggregates statistics.
    Returns:
        - total_proxies_found_before_dedup: Total configs found across all channels before final deduplication.
        - channels_processed_count: Number of channels processed (regardless of success/failure).
        - protocol_counts: Counts of each protocol among unique proxies.
        - all_unique_proxies: List of unique ProxyParsedConfig objects.
        - channel_status_counts: Counts of channel processing outcomes (success, empty_or_failed, critical_error).
        - quality_category_counts: Counts of quality categories among unique proxies.
    """
    channels_processed_count = 0
    total_proxies_found_before_dedup = 0
    # –°—Ç–∞—Ç—É—Å—ã: success (–Ω–∞—à–ª–∏ >0 –ø—Ä–æ–∫—Å–∏), empty_or_failed (0 –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ –æ—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è/–ø–∞—Ä—Å–∏–Ω–≥–∞), critical_error (–æ—à–∏–±–∫–∞ —Å–∞–º–æ–π –∑–∞–¥–∞—á–∏)
    channel_status_counts: DefaultDict[str, int] = defaultdict(int)
    all_proxies_nested: List[List[ProxyParsedConfig]] = [] # –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–æ–∫—Å–∏ —Å –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞

    channel_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_CHANNELS)

    async def task_wrapper(url):
        # –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –∑–∞–¥–∞—á–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–µ–º–∞—Ñ–æ—Ä–∞
        nonlocal channels_processed_count
        async with channel_semaphore:
            try:
                result = await process_channel_task(url, session, resolver)
                channels_processed_count += 1 # –°—á–∏—Ç–∞–µ–º –∫–∞–Ω–∞–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º
                return result
            except Exception as e:
                # –õ–æ–≤–∏–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –Ω–µ –ø–æ–π–º–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ process_channel_task
                logger.error(f"Critical task failure for {url}: {e}", exc_info=True)
                channels_processed_count += 1 # –°—á–∏—Ç–∞–µ–º –∫–∞–Ω–∞–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º (—Å –æ—à–∏–±–∫–æ–π)
                return e # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Å—Ç–∞—Ç—É—Å–∞

    tasks = [asyncio.create_task(task_wrapper(channel_url)) for channel_url in channel_urls]
    channel_results = await asyncio.gather(*tasks) # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å–ø–∏—Å–∫–∏ –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è)

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for result in channel_results:
        if isinstance(result, Exception):
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
            channel_status_counts["critical_error"] += 1
        elif isinstance(result, list):
            # –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç process_channel_task (—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏)
            all_proxies_nested.append(result)
            if result: # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
                channel_status_counts["success"] += 1
                total_proxies_found_before_dedup += len(result)
            else: # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ (–∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –∏–ª–∏ –æ—à–∏–±–∫–∏ –≤–Ω—É—Ç—Ä–∏ process_channel_task)
                channel_status_counts["empty_or_failed"] += 1
        else:
             logger.warning(f"Unexpected result type from gather: {type(result)}")
             channel_status_counts["unknown_error"] += 1


    # –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É—è __hash__ –∏ __eq__ –∏–∑ ProxyParsedConfig)
    unique_proxies_set: Set[ProxyParsedConfig] = set()
    for proxy_list in all_proxies_nested:
        unique_proxies_set.update(proxy_list)

    all_unique_proxies: List[ProxyParsedConfig] = sorted(list(unique_proxies_set), key=lambda p: p.quality_score, reverse=True) # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    logger.info(f"Total unique proxies found after deduplication: {len(all_unique_proxies)}")

    # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –£–ù–ò–ö–ê–õ–¨–ù–´–ú –ø—Ä–æ–∫—Å–∏
    protocol_counts: DefaultDict[str, int] = defaultdict(int)
    quality_category_counts: DefaultDict[str, int] = defaultdict(int)
    for proxy in all_unique_proxies:
        protocol_counts[proxy.protocol] += 1
        quality_category = get_quality_category(proxy.quality_score)
        quality_category_counts[quality_category] += 1

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    return (total_proxies_found_before_dedup,
            channels_processed_count,
            protocol_counts,
            all_unique_proxies,
            channel_status_counts,
            quality_category_counts)


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---

def output_statistics(start_time: float, total_channels_requested: int, channels_processed_count: int,
                      channel_status_counts: DefaultDict[str, int], total_proxies_found_before_dedup: int,
                      all_proxies_saved_count: int, protocol_counts: DefaultDict[str, int],
                      quality_category_counts: DefaultDict[str, int], # <-- –ü—Ä–∏–Ω–∏–º–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                      output_file: str):
    """Outputs download and processing statistics."""
    end_time = time.time()
    elapsed_time = end_time - start_time

    colored_log(logging.INFO, "==================== üìä PROXY DOWNLOAD STATISTICS ====================")
    colored_log(logging.INFO, f"‚è±Ô∏è  Script runtime: {elapsed_time:.2f} seconds")
    colored_log(logging.INFO, f"üîó Total channel URLs requested: {total_channels_requested}")
    colored_log(logging.INFO, f"üõ†Ô∏è Total channels processed (attempted): {channels_processed_count}/{total_channels_requested}")

    colored_log(logging.INFO, "\nüìä Channel Processing Status:")
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç—É—Å–æ–≤
    status_order = ["success", "empty_or_failed", "critical_error", "unknown_error"]
    status_colors = {
        "success": '\033[92m', # GREEN
        "empty_or_failed": '\033[93m', # YELLOW
        "critical_error": '\033[91m', # RED
        "unknown_error": '\033[91m', # RED
    }
    status_texts = {
        "success": "SUCCESS (found proxies)",
        "empty_or_failed": "EMPTY / FAILED (0 proxies)",
        "critical_error": "CRITICAL TASK ERROR",
        "unknown_error": "UNKNOWN ERROR",
    }

    for status_key in status_order:
        count = channel_status_counts.get(status_key, 0)
        if count > 0:
            color_start = status_colors.get(status_key, '\033[0m')
            status_text = status_texts.get(status_key, status_key.upper())
            colored_log(logging.INFO, f"  - {color_start}{status_text}{COLOR_MAP['RESET']}: {count} channels")

    colored_log(logging.INFO, f"\n‚ú® Total configurations found (before deduplication): {total_proxies_found_before_dedup}")
    colored_log(logging.INFO, f"üìù Total unique proxies saved: {all_proxies_saved_count} (to {output_file})")

    colored_log(logging.INFO, "\nüî¨ Protocol Breakdown (unique proxies):")
    if protocol_counts:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        for protocol, count in sorted(protocol_counts.items()):
            colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
    else:
        colored_log(logging.INFO, "   No protocol statistics available.")

    colored_log(logging.INFO, "\n‚≠êÔ∏è Proxy Quality Category Distribution (unique proxies):")
    if quality_category_counts:
         # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (High, Medium, Low)
         category_order = {"High": 0, "Medium": 1, "Low": 2, "Unknown": 3}
         for category, count in sorted(quality_category_counts.items(), key=lambda item: category_order.get(item[0], 99)):
             colored_log(logging.INFO, f"   - {category}: {count} proxies")
    else:
        colored_log(logging.INFO, "   No quality category statistics available.")

    colored_log(logging.INFO, "======================== üèÅ STATISTICS END =========================")


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è main —Ñ—É–Ω–∫—Ü–∏—è ---

async def main() -> None:
    """Main function to run the proxy downloader script."""
    parser = argparse.ArgumentParser(description="Proxy Downloader Script")
    parser.add_argument('--nocolorlogs', action='store_true', help='Disable colored console logs')
    args = parser.parse_args()

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ü–≤–µ—Ç–∞ –∫ —Ñ–æ—Ä–º–∞—Ç–µ—Ä—Ä—É
    console_formatter.use_colors = not args.nocolorlogs

    try:
        start_time = time.time()
        channel_urls = await load_channel_urls(CONFIG_FILES.ALL_URLS)
        total_channels_requested = len(channel_urls) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

        if not channel_urls:
            colored_log(logging.WARNING, "No channel URLs to process.")
            return

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–æ–ª–≤–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ aiodns.DNSResolver –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ close()
        resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é aiohttp –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
        async with aiohttp.ClientSession() as session:
            # –í—ã–∑—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
            (total_proxies_found_before_dedup, channels_processed_count,
             protocol_counts, all_unique_proxies, channel_status_counts,
             quality_category_counts) = await load_and_process_channels(
                channel_urls, session, resolver)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏
        all_proxies_saved_count = save_unique_proxies_to_file(all_unique_proxies, CONFIG_FILES.OUTPUT_ALL_CONFIG)

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –ø–µ—Ä–µ–¥–∞–≤–∞—è –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
        output_statistics(start_time, total_channels_requested, channels_processed_count,
                          channel_status_counts, total_proxies_found_before_dedup,
                          all_proxies_saved_count, protocol_counts, quality_category_counts,
                          CONFIG_FILES.OUTPUT_ALL_CONFIG)

    except Exception as e:
        # –õ–æ–≤–∏–º –ª—é–±—ã–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ
        logger.critical(f"Unexpected critical error in main execution: {e}", exc_info=True)
        sys.exit(1) # –í—ã—Ö–æ–¥–∏–º —Å –∫–æ–¥–æ–º –æ—à–∏–±–∫–∏
    finally:
        # –≠—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –±—É–¥–µ—Ç –≤—ã–≤–µ–¥–µ–Ω–æ –≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        colored_log(logging.INFO, "‚úÖ Proxy download and processing script finished.")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é main —Ñ—É–Ω–∫—Ü–∏—é
    asyncio.run(main())
