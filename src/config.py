import asyncio
import aiodns
import os
import logging
import ipaddress
import time
import json
import functools
from enum import Enum
from urllib.parse import urlparse, parse_qs
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
import aiohttp

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º coloredlogs) ---
import coloredlogs

LOG_FILE = 'proxy_downloader.log'
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–∞ DEBUG

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–∞ (—É—Ä–æ–≤–µ–Ω—å WARNING –∏ –≤—ã—à–µ, —Ñ–æ—Ä–º–∞—Ç JSON)
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)

class JsonFormatter(logging.Formatter):
    """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤ –≤ JSON."""
    def format(self, record):
        log_record = {
            "time": self.formatTime(record, self.default_time_format),
            "level": record.levelname,
            "message": record.getMessage(),
            "process": record.process,
            "module": record.module,
            "funcName": record.funcName,
            "lineno": record.lineno,
        }
        if record.exc_info:
            log_record['exc_info'] = self.formatException(record.exc_info)
        return json.dumps(log_record, ensure_ascii=False)

formatter_file = JsonFormatter()
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–Ω—Å–æ–ª–∏ (—É—Ä–æ–≤–µ–Ω—å INFO, —Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é coloredlogs)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
coloredlogs.install(level='INFO', logger=logger, stream=console_handler,
                    fmt='[%(levelname)s] %(message)s')  # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è coloredlogs

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è ---
class Protocols(str, Enum):
    VLESS = "vless"
    TUIC = "tuic"
    HY2 = "hy2"
    SS = "ss"
    SSR = "ssr"
    TROJAN = "trojan"

@dataclass(frozen=True)
class ConfigFiles:
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt"

@dataclass(frozen=True)
class RetrySettings:
    MAX_RETRIES: int = 4
    RETRY_DELAY_BASE: int = 2

@dataclass(frozen=True)
class ConcurrencyLimits:
    MAX_CHANNELS: int = 60
    MAX_PROXIES_PER_CHANNEL: int = 50
    MAX_PROXIES_GLOBAL: int = 50

ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]
CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()

# --- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è ---
class InvalidURLError(ValueError):
    """–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π URL-–∞–¥—Ä–µ—Å."""
    pass

class UnsupportedProtocolError(ValueError):
    """–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª."""
    pass

class DownloadError(Exception):
    """–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏."""
    pass

# --- –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ---
@dataclass(frozen=True)
class ProxyParsedConfig:
    config_string: str
    protocol: str
    address: str
    port: int
    remark: str = ""
    query_params: Dict[str, str] = field(default_factory=dict)

    def __hash__(self):
        return hash((self.protocol, self.address, self.port))

    def __str__(self):
        return (f"ProxyConfig(protocol={self.protocol}, address={self.address}, "
                f"port={self.port}, config_string='{self.config_string[:50]}...')")

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        """–†–∞–∑–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –≤ –æ–±—ä–µ–∫—Ç ProxyParsedConfig."""
        if len(config_string) > 1024: # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É config_string
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π URL: {config_string[:100]}...")
            return None

        protocol = next((p for p in ALLOWED_PROTOCOLS if config_string.startswith(p + "://")), None)
        if not protocol:
            try:
                decoded_config = base64.b64decode(config_string, validate=True).decode('utf-8', errors='ignore')
                protocol = next((p for p in ALLOWED_PROTOCOLS if decoded_config.startswith(p + "://")), None)
                if protocol:
                    config_string = decoded_config
                else:
                    return None
            except:
                return None

        try:
            parsed_url = urlparse(config_string)
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port:
                return None

            remark = parsed_url.fragment if parsed_url.fragment else ""
            query_params = {k: v[0] for k, v in parse_qs(parsed_url.query).items()} if parsed_url.query else {}

            return cls(
                config_string=config_string.split("#")[0],
                protocol=protocol,
                address=address,
                port=port,
                remark=remark,
                query_params=query_params,
            )
        except ValueError:
            return None

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–º IPv4-–∞–¥—Ä–µ—Å–æ–º."""
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """–†–∞–∑—Ä–µ—à–∞–µ—Ç –∏–º—è —Ö–æ—Å—Ç–∞ –≤ IPv4-–∞–¥—Ä–µ—Å."""
    if is_valid_ipv4(hostname):
        return hostname

    try:
        async with asyncio.timeout(10):
            result = await resolver.query(hostname, 'A')
            resolved_ip = result[0].host
            return resolved_ip if is_valid_ipv4(resolved_ip) else None
    except (asyncio.TimeoutError, aiodns.error.DNSError) as e:
        logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è DNS –¥–ª—è {hostname}: {e}", stacklevel=2)
        return None
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ DNS –¥–ª—è {hostname}: {e}", exc_info=True, stacklevel=2)
        return None

# --- –§—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession, channel_proxy_semaphore: asyncio.Semaphore) -> Tuple[List[str], str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–∞."""
    headers = {'User-Agent': 'ProxyDownloader/1.0'}
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=15)

    while retries_attempted <= RETRY.MAX_RETRIES:
        try:
            async with channel_proxy_semaphore: # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                async with session.get(channel_url, timeout=session_timeout, headers=headers) as response:
                    response.raise_for_status()
                    text = await response.text(encoding='utf-8', errors='ignore')

                    if not text.strip():
                        logger.warning(f"–ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.", stacklevel=2)
                        return [], "warning"

                    try:
                        decoded_text = base64.b64decode(text.strip(), validate=True).decode('utf-8', errors='ignore')
                        return decoded_text.splitlines(), "success"
                    except:
                        return text.splitlines(), "success"

        except aiohttp.ClientResponseError as e:
            logger.warning(f"–ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª HTTP –æ—à–∏–±–∫—É {e.status}: {e.message}", stacklevel=2)
            return [], "warning"
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted)
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {channel_url} (–ø–æ–ø—ã—Ç–∫–∞ {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...", stacklevel=2)
            if retries_attempted == RETRY.MAX_RETRIES:
                logger.error(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({RETRY.MAX_RETRIES+1}) –¥–ª—è {channel_url}", stacklevel=2)
                return [], "error"
            await asyncio.sleep(retry_delay)
        retries_attempted += 1

    return [], "critical"

async def parse_and_filter_proxies(lines: List[str], resolver: aiodns.DNSResolver) -> List[ProxyParsedConfig]:
    """–†–∞–∑–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏."""
    parsed_configs = []
    processed_configs = set()

    for line in lines:
        line = line.strip()
        if not line:
            continue

        parsed_config = ProxyParsedConfig.from_url(line)
        if parsed_config is None:
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–∫—Å–∏ URL: {line}", stacklevel=2)  # –õ–æ–≥–∏—Ä—É–µ–º
            continue

        if parsed_config.config_string in processed_configs:
            continue
        processed_configs.add(parsed_config.config_string)

        resolved_ip = await resolve_address(parsed_config.address, resolver)
        if resolved_ip:
            parsed_configs.append(parsed_config)

    return parsed_configs

def generate_proxy_profile_name(proxy_config: ProxyParsedConfig) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è –ø—Ä–æ–∫—Å–∏."""
    protocol = proxy_config.protocol.upper()
    type_ = proxy_config.query_params.get('type', 'unknown').lower()
    security = proxy_config.query_params.get('security', 'none').lower()
    if protocol == 'SS' and type_ == 'unknown':
        type_ = 'tcp'
    return f"{protocol}_{type_}_{security}"

async def save_proxies_from_queue(queue: asyncio.Queue, output_file: str) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –≤ —Ñ–∞–π–ª (—Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)."""
    total_proxies_count = 0
    seen_config_strings = set()
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            while True:
                proxy_conf = await queue.get()
                if proxy_conf is None:  # –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    break
                if proxy_conf.config_string not in seen_config_strings:
                    seen_config_strings.add(proxy_conf.config_string)
                    profile_name = generate_proxy_profile_name(proxy_conf)
                    config_line = f"{proxy_conf.config_string}#{profile_name}"
                    f.write(config_line + "\n")
                    total_proxies_count += 1
                queue.task_done()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –≤ —Ñ–∞–π–ª: {e}", exc_info=True, stacklevel=2)
    return total_proxies_count

async def load_channel_urls(all_urls_file: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    channel_urls = []
    try:
        with open(all_urls_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and _is_valid_url(url):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
                    channel_urls.append(url)
                elif url:
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π URL –∫–∞–Ω–∞–ª–∞: {url}", stacklevel=2)
    except FileNotFoundError:
        logger.warning(f"–§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª.", stacklevel=2)
        open(all_urls_file, 'w').close()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è/—á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {all_urls_file}: {e}", exc_info=True, stacklevel=2)
    return channel_urls

def _is_valid_url(url: str) -> bool:
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ URL."""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False

async def process_channel(url: str, session: aiohttp.ClientSession, resolver: aiodns.DNSResolver, proxy_queue: asyncio.Queue, channel_proxy_semaphore: asyncio.Semaphore) -> Tuple[int, bool]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–Ω–∞–ª."""
    logger.info(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: {url}", stacklevel=2)
    lines, status = await download_proxies_from_channel(url, session, channel_proxy_semaphore)
    if status == "success":
        parsed_proxies = await parse_and_filter_proxies(lines, resolver)
        channel_proxies_count = len(parsed_proxies)
        for proxy in parsed_proxies:
            await proxy_queue.put(proxy)
        logger.info(f"‚úÖ –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ù–∞–π–¥–µ–Ω–æ {channel_proxies_count} –ø—Ä–æ–∫—Å–∏.", stacklevel=2)
        return channel_proxies_count, True
    else:
        logger.warning(f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {status}.", stacklevel=2)
        return 0, False

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    start_time = time.time()
    channel_urls = await load_channel_urls(CONFIG_FILES.ALL_URLS)
    if not channel_urls:
        logger.warning("–ù–µ—Ç URL-–∞–¥—Ä–µ—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.", stacklevel=2)
        return

    total_channels = len(channel_urls)
    channels_processed_successfully = 0
    total_proxies_downloaded = 0
    protocol_counts = defaultdict(int)
    channel_status_counts = defaultdict(int)

    resolver = aiodns.DNSResolver()
    proxy_queue = asyncio.Queue()
    channel_proxy_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_PROXIES_PER_CHANNEL)

    try:
        async with aiohttp.ClientSession() as session:
            async with asyncio.TaskGroup() as tg:
                channel_tasks = [tg.create_task(process_channel(url, session, resolver, proxy_queue, channel_proxy_semaphore)) for url in channel_urls]

            channel_results = [task.result() for task in channel_tasks]  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ

            for proxies_count, success_flag in channel_results:
                total_proxies_downloaded += proxies_count
                channels_processed_successfully += int(success_flag) # –Ø–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
                # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –≤–µ–¥–µ—Ç—Å—è *–ø–æ—Å–ª–µ* –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤
                # (—á—Ç–æ–±—ã —É—á–µ—Å—Ç—å –≤—Å–µ –ø—Ä–æ–∫—Å–∏, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –æ—á–µ—Ä–µ–¥—å)

            await proxy_queue.join()  # –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—á–µ—Ä–µ–¥—å –æ–ø—É—Å—Ç–µ–µ—Ç
            await proxy_queue.put(None)  # –ü–æ—Å—ã–ª–∞–µ–º —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            save_task = asyncio.create_task(save_proxies_from_queue(proxy_queue, CONFIG_FILES.OUTPUT_ALL_CONFIG))
            all_proxies_saved_count = await save_task

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª
            for proxy in [item for q in channel_results for item in (await parse_and_filter_proxies(await download_proxies_from_channel(q[2], session, channel_proxy_semaphore)[0], resolver)) if item]:
               protocol_counts[proxy.protocol] += 1
            channel_status_counts = defaultdict(int, {k: sum(1 for r in channel_results if r[1] == (k == "success")) for k in ["success", "warning", "error", "critical"]})


    except Exception as e:
        logger.critical(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}", exc_info=True, stacklevel=2)
    finally:
        logger.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.", stacklevel=2)


    end_time = time.time()
    elapsed_time = end_time - start_time

    # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å ---
    logger.info("==================== üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–û–ö–°–ò ====================", stacklevel=2)
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {elapsed_time:.2f} —Å–µ–∫", stacklevel=2)
    logger.info(f"üîó –í—Å–µ–≥–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_channels}", stacklevel=2)
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {channels_processed_successfully}/{total_channels}", stacklevel=2)

    logger.info("\nüìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:", stacklevel=2)
    for status_key in ["success", "warning", "error", "critical"]:
        count = channel_status_counts.get(status_key, 0)
        if count > 0:
            if status_key == "success":
                status_text = "–£–°–ü–ï–®–ù–û"
            elif status_key == "warning":
                status_text = "–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï"
            elif status_key in ["error", "critical"]:
                status_text = "–û–®–ò–ë–ö–ê"
            else:
                status_text = status_key.upper()
            logger.info(f"  - {status_text}: {count} –∫–∞–Ω–∞–ª–æ–≤", stacklevel=2)

    logger.info(f"\n‚ú® –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {total_proxies_downloaded}", stacklevel=2)
    logger.info(f"üìù –í—Å–µ–≥–æ –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ, –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {all_proxies_saved_count} (–≤ {CONFIG_FILES.OUTPUT_ALL_CONFIG})", stacklevel=2)

    logger.info("\nüî¨ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º (–Ω–∞–π–¥–µ–Ω–æ):", stacklevel=2)
    if protocol_counts:
        for protocol, count in protocol_counts.items():
            logger.info(f"   - {protocol.upper()}: {count}", stacklevel=2)
    else:
        logger.info("   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.", stacklevel=2)

    logger.info("======================== üèÅ –ö–û–ù–ï–¶ –°–¢–ê–¢–ò–°–¢–ò–ö–ò =========================", stacklevel=2)
if __name__ == "__main__":
    asyncio.run(main())
