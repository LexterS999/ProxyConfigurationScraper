import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import io
import uuid
import string
import base64
import aiohttp
import time
import json
import functools

from enum import Enum
from urllib.parse import urlparse, parse_qs, urlsplit
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict


# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
LOG_FORMAT = {"time": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s", "process": "%(process)s"}
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"
LOG_FILE = 'proxy_downloader.log'

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)

class JsonFormatter(logging.Formatter): # –ö–∞—Å—Ç–æ–º–Ω—ã–π JSON formatter
    def format(self, record):
        log_record = LOG_FORMAT.copy()
        log_record["message"] = record.getMessage() # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        log_record["level"] = record.levelname
        log_record["process"] = record.process
        log_record["time"] = self.formatTime(record, self.default_time_format) # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
        return json.dumps(log_record, ensure_ascii=False) # JSON dump

formatter_file = JsonFormatter() # –ò—Å–ø–æ–ª—å–∑—É–µ–º JSON formatter
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter_console = logging.Formatter(CONSOLE_LOG_FORMAT)
console_handler.setFormatter(formatter_console)
logger.addHandler(console_handler)


def colored_log(level, message: str, *args, **kwargs):
    RESET = '\033[0m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BOLD_RED = '\033[1m\033[91m' # Bold Red

    color = RESET
    if level == logging.INFO:
        color = GREEN
    elif level == logging.WARNING:
        color = YELLOW
    elif level == logging.ERROR:
        color = RED
    elif level == logging.CRITICAL:
        color = BOLD_RED # –ò—Å–ø–æ–ª—å–∑—É–µ–º BOLD_RED

    record = logging.LogRecord(
        name=logger.name,
        level=level,
        pathname='proxy_downloader.py', # –ò–ª–∏ __file__ –µ—Å–ª–∏ –≤ –º–æ–¥—É–ª–µ
        lineno=0, # –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏ –æ—Ç–∫—É–¥–∞ –≤—ã–∑–≤–∞–Ω–æ, –Ω–æ —Å–µ–π—á–∞—Å 0
        msg=f"{color}{message}{RESET}",
        args=args,
        exc_info=kwargs.get('exc_info'), # –ü–µ—Ä–µ–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–∫–ª—é—á–µ–Ω–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        func='colored_log', # –ò–º—è —Ñ—É–Ω–∫—Ü–∏–∏
        sinfo=None # Stack info
    )
    logger.handle(record) # –ò—Å–ø–æ–ª—å–∑—É–µ–º handle –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ LogRecord


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
from enum import Enum
from dataclasses import dataclass

class Protocols(Enum):
    VLESS = "vless://"
    TUIC = "tuic://"
    HY2 = "hy2://"
    SS = "ss://"

@dataclass(frozen=True)
class ConfigFiles:
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt"

@dataclass(frozen=True)
class RetrySettings:
    MAX_RETRIES: int = 4
    RETRY_DELAY_BASE: int = 2

@dataclass(frozen=True)
class ConcurrencyLimits:
    MAX_CHANNELS: int = 60
    MAX_PROXIES_PER_CHANNEL: int = 50
    MAX_PROXIES_GLOBAL: int = 50

ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]
CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()

OUTPUT_ALL_CONFIG_FILE = os.path.join("configs", "proxy_configs_all.txt")
ALL_URLS_FILE = "channel_urls.txt" # –ò–ª–∏ os.path.join(".") –¥–ª—è —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏


class ProfileName(Enum):
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    UNKNOWN = "Unknown Protocol"

class InvalidURLError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º–æ–µ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ URL."""
    pass

class UnsupportedProtocolError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º–æ–µ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞."""
    pass

@dataclass(frozen=True)
class ProxyParsedConfig:
    config_string: str
    protocol: str
    address: str
    port: int

    def __hash__(self):
        return hash((self.protocol, self.address, self.port))

    def __str__(self):
        return f"ProxyConfig(protocol={self.protocol}, address={self.address}, port={self.port}, config_string='{self.config_string[:50]}...')" # –û–±—Ä–µ–∑–∞–µ–º config_string –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏

    @classmethod
    def from_url(cls, config_string: str) -> "ProxyParsedConfig": # –£–±–∏—Ä–∞–µ–º Optional, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
        protocol = next((p for p in ALLOWED_PROTOCOLS if config_string.startswith(p)), None)
        if not protocol:
            raise UnsupportedProtocolError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –≤ URL: {config_string}") # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ

        try:
            parsed_url = urlparse(config_string)
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port:
                raise InvalidURLError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–ª–∏ –ø–æ—Ä—Ç –∏–∑ URL: {config_string}") # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            return cls(
                config_string=config_string,
                protocol=protocol.replace("://", ""),
                address=address,
                port=port
            )
        except ValueError as e:
            raise InvalidURLError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ URL: {config_string}. –û—à–∏–±–∫–∞: {e}") from e # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º


async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """Resolves a hostname to an IPv4 address using DNS."""
    if is_valid_ipv4(hostname):
        return hostname
    try:
        async with asyncio.timeout(10): # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç 10 —Å–µ–∫—É–Ω–¥
            result = await resolver.query(hostname, 'A')
            resolved_address = result[0].host
            if is_valid_ipv4(resolved_address):
                return resolved_address
            else:
                colored_log(logging.WARNING, f"‚ö†Ô∏è DNS resolved {hostname} to non-IPv4 address: {resolved_address}") # –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ-IPv4
                return None
    except asyncio.TimeoutError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è DNS resolution timed out for {hostname}") # –õ–æ–≥–∏—Ä—É–µ–º —Ç–∞–π–º–∞—É—Ç
        return None
    except aiodns.error.DNSError as e:
        colored_log(logging.WARNING, f"‚ö†Ô∏è DNS resolution failed for {hostname}: {e}") # –ë–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        return None
    except Exception as e: # –õ–æ–≤–∏–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ DNS resolution –¥–ª—è {hostname}: {e}", exc_info=True) # –õ–æ–≥–∏—Ä—É–µ–º —Å traceback
        return None

@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ hostname –≤–∞–ª–∏–¥–Ω—ã–º IPv4 –∞–¥—Ä–µ—Å–æ–º."""
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> Tuple[List[str], str]:
    """Downloads proxy configurations from a single channel URL with retry logic."""
    headers = {'User-Agent': 'ProxyDownloader/1.0'} # –î–æ–±–∞–≤–ª—è–µ–º User-Agent
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=15)
    while retries_attempted <= RETRY.MAX_RETRIES: # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –∏–∑ RetrySettings
        try:
            async with session.get(channel_url, timeout=session_timeout, headers=headers) as response: # –ü–µ—Ä–µ–¥–∞–µ–º headers –≤ get
                response.raise_for_status() # –í—ã–±—Ä–æ—Å–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ 4xx –∏ 5xx
                text = await response.text(encoding='utf-8', errors='ignore')
                return text.splitlines(), "success"
        except aiohttp.ClientResponseError as e: # –õ–æ–≤–∏–º –∏–º–µ–Ω–Ω–æ ClientResponseError
            colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª HTTP –æ—à–∏–±–∫—É {e.status}: {e.message}")
            return [], "warning" # Treat as warning, don't retry immediately for HTTP errors
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
            colored_log(logging.WARNING, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {channel_url} (–ø–æ–ø—ã—Ç–∫–∞ {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {e}. –ü–∞—É–∑–∞ {retry_delay} —Å–µ–∫")
            if retries_attempted == RETRY.MAX_RETRIES: # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
                colored_log(logging.ERROR, f"‚ùå –ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ ({RETRY.MAX_RETRIES+1}) –∏—Å—á–µ—Ä–ø–∞–Ω–æ –¥–ª—è {channel_url}")
                return [], "error"
            await asyncio.sleep(retry_delay)
        retries_attempted += 1
    return [], "critical" # Should not reach here, but for type hinting

async def parse_and_filter_proxies(lines: List[str], resolver: aiodns.DNSResolver) -> List[ProxyParsedConfig]:
    """Parses and filters valid proxy configurations from lines, resolving addresses."""
    parsed_configs = []
    for line in lines:
        line = line.strip()
        if not line or not any(line.startswith(proto) for proto in ALLOWED_PROTOCOLS):
            continue
        try:
            parsed_config = ProxyParsedConfig.from_url(line) # from_url —Ç–µ–ø–µ—Ä—å –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        except (InvalidURLError, UnsupportedProtocolError) as e: # –õ–æ–≤–∏–º –Ω–∞—à–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            colored_log(logging.WARNING, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ URL '{line}': {e}") # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞
            continue # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–æ–∫–µ
        if parsed_config:
            resolved_ip = await resolve_address(parsed_config.address, resolver)
            if resolved_ip:
                 parsed_configs.append(parsed_config)
    return parsed_configs

def save_all_proxies_to_file(all_proxies: List[ProxyParsedConfig], output_file: str) -> int:
    """Saves all downloaded proxies to the output file, grouped by protocol."""
    total_proxies_count = 0
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            protocol_grouped_proxies = defaultdict(list)
            for proxy_conf in all_proxies:
                protocol_grouped_proxies[proxy_conf.protocol].append(proxy_conf)

            for protocol_name in ProfileName: # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ Enum ProfileName
                protocol = protocol_name.name.lower() # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                if protocol in protocol_grouped_proxies:
                    colored_log(logging.INFO, f"\nüìù –ü—Ä–æ—Ç–æ–∫–æ–ª (–≤—Å–µ): {protocol_name.value}") # –ò—Å–ø–æ–ª—å–∑—É–µ–º value –∏–∑ Enum
                    for proxy_conf in protocol_grouped_proxies[protocol]:
                        config_line = proxy_conf.config_string + f"#{protocol_name.value}" # –ò—Å–ø–æ–ª—å–∑—É–µ–º value –∏–∑ Enum
                        f.write(config_line + "\n")
                        colored_log(logging.INFO, f"   ‚ûï –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ): {config_line}")
                        total_proxies_count += 1
        colored_log(logging.INFO, f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_proxies_count} –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ) –≤ {output_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª: {e}", exc_info=True)
    return total_proxies_count


async def load_channel_urls(all_urls_file: str) -> List[str]:
    """Loads channel URLs from the specified file."""
    channel_urls = []
    try:
        with open(all_urls_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url:
                    channel_urls.append(url)
    except FileNotFoundError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è –§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å URL –∫–∞–Ω–∞–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞.") # –£—Ç–æ—á–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        open(all_urls_file, 'w').close() # Create empty file if not exists
    except Exception as e: # –õ–æ–≤–∏–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–∞–π–ª–∞
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Ñ–∞–π–ª–∞ {all_urls_file}: {e}", exc_info=True) # –õ–æ–≥–∏—Ä—É–µ–º —Å traceback
    return channel_urls


async def main():
    """Main function to download and process proxy configurations."""
    try:
        start_time = time.time()
        channel_urls = await load_channel_urls(ALL_URLS_FILE)
        if not channel_urls:
            colored_log(logging.WARNING, "–ù–µ—Ç URL –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        total_channels = len(channel_urls)
        channels_processed_successfully = 0
        total_proxies_downloaded = 0
        protocol_counts = defaultdict(int)
        channel_status_counts = defaultdict(int)

        resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())
        global_proxy_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_PROXIES_GLOBAL) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
        channel_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_CHANNELS) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É

        async with aiohttp.ClientSession() as session:
            channel_tasks = []
            for channel_url in channel_urls:
                async def process_channel_task(url):
                    channel_proxies_count_channel = 0 # Initialize count here
                    channel_success = 0 # Initialize success count
                    async with channel_semaphore:
                        colored_log(logging.INFO, f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞: {url}")
                        lines, status = await download_proxies_from_channel(url, session)
                        channel_status_counts[status] += 1
                        if status == "success":
                            parsed_proxies = await parse_and_filter_proxies(lines, resolver)
                            channel_proxies_count_channel = len(parsed_proxies)
                            channel_success = 1 # Mark channel as success after processing
                            for proxy in parsed_proxies:
                                protocol_counts[proxy.protocol] += 1
                            colored_log(logging.INFO, f"‚úÖ –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ù–∞–π–¥–µ–Ω–æ {channel_proxies_count_channel} –ø—Ä–æ–∫—Å–∏.")
                            return channel_proxies_count_channel, channel_success, parsed_proxies # Return counts and proxies
                        else:
                            colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {status}.")
                            return 0, 0, [] # Return zero counts and empty list for failed channels

                task = asyncio.create_task(process_channel_task(channel_url))
                channel_tasks.append(task)

            channel_results = await asyncio.gather(*channel_tasks)
            all_proxies = []
            for proxies_count, success_flag, proxies_list in channel_results: # Unpack returned values
                total_proxies_downloaded += proxies_count # Aggregate proxy counts
                channels_processed_successfully += success_flag # Aggregate success flags
                all_proxies.extend(proxies_list) # Collect proxies

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã) –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        all_proxies_saved_count = save_all_proxies_to_file(all_proxies, OUTPUT_ALL_CONFIG_FILE)
        end_time = time.time()
        elapsed_time = end_time - start_time

        colored_log(logging.INFO, "==================== üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–û–ö–°–ò ====================")
        colored_log(logging.INFO, f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {elapsed_time:.2f} —Å–µ–∫")
        colored_log(logging.INFO, f"üîó –í—Å–µ–≥–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_channels}")
        colored_log(logging.INFO, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {channels_processed_successfully}/{total_channels}") # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É—Å–ø–µ—à–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤

        colored_log(logging.INFO, "\nüìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
        for status in ["success", "warning", "error", "critical"]:
            count = channel_status_counts.get(status, 0)
            if count > 0:
                status_text = status.upper()
                color = '\033[92m' if status == "success" else ('\033[93m' if status == "warning" else ('\033[91m' if status in ["error", "critical"] else '\033[0m'))
                colored_log(logging.INFO, f"  - {color}{status_text}\033[0m: {count} –∫–∞–Ω–∞–ª–æ–≤")

        colored_log(logging.INFO, f"\n‚ú® –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {total_proxies_downloaded}")
        colored_log(logging.INFO, f"üìù –í—Å–µ–≥–æ –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {all_proxies_saved_count} (–≤ {OUTPUT_ALL_CONFIG_FILE})")


        colored_log(logging.INFO, "\nüî¨ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º (–Ω–∞–π–¥–µ–Ω–æ):")
        if protocol_counts:
            for protocol, count in protocol_counts.items():
                colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
        else:
            colored_log(logging.INFO, "   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.")

        colored_log(logging.INFO, "======================== üèÅ –ö–û–ù–ï–¶ –°–¢–ê–¢–ò–°–¢–ò–ö–ò =========================")

    except Exception as e:
        logger.critical(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}", exc_info=True) # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É —Å traceback
    finally: # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ "–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞." –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        colored_log(logging.INFO, "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    asyncio.run(main())
