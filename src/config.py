import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import io
import uuid
import string
import base64
import aiohttp
import time

from enum import Enum
from urllib.parse import urlparse, parse_qs, urlsplit
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict
import functools

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
LOG_FORMAT = "%(asctime)s [%(levelname)s] %(message)s (Process: %(process)s)"
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"
LOG_FILE = 'proxy_downloader.log'

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)
formatter_file = logging.Formatter(LOG_FORMAT)
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter_console = logging.Formatter(CONSOLE_LOG_FORMAT)
console_handler.setFormatter(formatter_console)
logger.addHandler(console_handler)

class LogColors:
    RESET = '\033[0m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def colored_log(level, message: str, *args, **kwargs):
    color = LogColors.RESET
    if level == logging.INFO:
        color = LogColors.GREEN
    elif level == logging.WARNING:
        color = LogColors.YELLOW
    elif level == logging.ERROR:
        color = LogColors.RED
    elif level == logging.CRITICAL:
        color = LogColors.BOLD + LogColors.RED
    logger.log(level, f"{color}{message}{LogColors.RESET}", *args, **kwargs)

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
ALLOWED_PROTOCOLS = ["vless://", "tuic://", "hy2://", "ss://"]
ALL_URLS_FILE = "channel_urls.txt" # –§–∞–π–ª —Å URL –∫–∞–Ω–∞–ª–æ–≤ (–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏)
OUTPUT_CONFIG_FILE = "proxy_configs_unique.txt" # –§–∞–π–ª –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏)
OUTPUT_ALL_CONFIG_FILE = "proxy_configs_all.txt" # –ù–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏
MAX_RETRIES = 3
RETRY_DELAY_BASE = 2
MAX_CONCURRENT_CHANNELS = 90
MAX_CONCURRENT_PROXIES_PER_CHANNEL = 120
MAX_CONCURRENT_PROXIES_GLOBAL = 240

class ProfileName(Enum):
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    UNKNOWN = "Unknown Protocol"

class InvalidURLError(ValueError):
    pass

class UnsupportedProtocolError(ValueError):
    pass

@dataclass(frozen=True)
class ProxyParsedConfig:
    config_string: str
    protocol: str
    address: str
    port: int

    def __hash__(self):
        return hash((self.protocol, self.address, self.port))

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        protocol = next((p for p in ALLOWED_PROTOCOLS if config_string.startswith(p)), None)
        if not protocol:
            return None

        try:
            parsed_url = urlparse(config_string)
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port:
                return None
            return cls(
                config_string=config_string,
                protocol=protocol.replace("://", ""),
                address=address,
                port=port
            )
        except ValueError:
            return None

async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    if is_valid_ipv4(hostname):
        return hostname
    try:
        result = await resolver.query(hostname, 'A')
        resolved_address = result[0].host
        if is_valid_ipv4(resolved_address):
            return resolved_address
        else:
            return None
    except aiodns.error.DNSError as e:
        return None
    except Exception:
        return None

@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> Tuple[List[str], str]:
    """Downloads proxy configurations from a single channel URL with retry logic."""
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=15)
    while retries_attempted <= MAX_RETRIES:
        try:
            async with session.get(channel_url, timeout=session_timeout) as response:
                if response.status == 200:
                    text = await response.text(encoding='utf-8', errors='ignore')
                    return text.splitlines(), "success"
                else:
                    colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return [], "warning" # Treat as warning, don't retry immediately for HTTP errors
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY_DELAY_BASE * (2 ** retries_attempted)
            colored_log(logging.WARNING, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {channel_url} (–ø–æ–ø—ã—Ç–∫–∞ {retries_attempted+1}/{MAX_RETRIES+1}): {e}. –ü–∞—É–∑–∞ {retry_delay} —Å–µ–∫")
            if retries_attempted == MAX_RETRIES:
                colored_log(logging.ERROR, f"‚ùå –ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ ({MAX_RETRIES+1}) –∏—Å—á–µ—Ä–ø–∞–Ω–æ –¥–ª—è {channel_url}")
                return [], "error"
            await asyncio.sleep(retry_delay)
        retries_attempted += 1
    return [], "critical" # Should not reach here, but for type hinting

async def parse_and_filter_proxies(lines: List[str], resolver: aiodns.DNSResolver) -> List[ProxyParsedConfig]:
    """Parses and filters valid proxy configurations from lines."""
    parsed_configs = []
    for line in lines:
        line = line.strip()
        if not line or not any(line.startswith(proto) for proto in ALLOWED_PROTOCOLS):
            continue
        parsed_config = ProxyParsedConfig.from_url(line)
        if parsed_config:
            resolved_ip = await resolve_address(parsed_config.address, resolver)
            if resolved_ip:
                 parsed_configs.append(parsed_config)
    return parsed_configs

def deduplicate_proxies(parsed_proxies: List[ProxyParsedConfig]) -> Dict[str, Set[ProxyParsedConfig]]:
    """Deduplicates proxies based on IP and port within each protocol."""
    unique_proxies_by_protocol = defaultdict(set)
    for proxy in parsed_proxies:
        unique_proxies_by_protocol[proxy.protocol].add(proxy)
    return unique_proxies_by_protocol

def save_proxies_to_file(unique_proxies_by_protocol: Dict[str, Set[ProxyParsedConfig]], output_file: str) -> int:
    """Saves unique proxies to the output file with protocol names."""
    total_proxies_count = 0
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            for protocol in ["vless", "tuic", "hy2", "ss"]: # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                if protocol in unique_proxies_by_protocol:
                    colored_log(logging.INFO, f"\nüõ°Ô∏è  –ü—Ä–æ—Ç–æ–∫–æ–ª (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ): {ProfileName[protocol.upper()].value}")
                    for proxy_conf in unique_proxies_by_protocol[protocol]:
                        config_line = proxy_conf.config_string + f"#{ProfileName[protocol.upper()].value}"
                        f.write(config_line + "\n")
                        colored_log(logging.INFO, f"   ‚ú® –î–æ–±–∞–≤–ª–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–∫—Å–∏: {config_line}")
                        total_proxies_count += 1
        colored_log(logging.INFO, f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_proxies_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –≤ {output_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª: {e}")
    return total_proxies_count

def save_all_proxies_to_file(all_proxies: List[ProxyParsedConfig], output_file: str) -> int:
    """Saves all downloaded proxies to the output file with protocol names (including duplicates)."""
    total_proxies_count = 0
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            protocol_grouped_proxies = defaultdict(list)
            for proxy_conf in all_proxies:
                protocol_grouped_proxies[proxy_conf.protocol].append(proxy_conf)

            for protocol in ["vless", "tuic", "hy2", "ss"]: # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                if protocol in protocol_grouped_proxies:
                    colored_log(logging.INFO, f"\nüìù –ü—Ä–æ—Ç–æ–∫–æ–ª (–≤—Å–µ): {ProfileName[protocol.upper()].value}")
                    for proxy_conf in protocol_grouped_proxies[protocol]:
                        config_line = proxy_conf.config_string + f"#{ProfileName[protocol.upper()].value}"
                        f.write(config_line + "\n")
                        colored_log(logging.INFO, f"   ‚ûï –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ): {config_line}")
                        total_proxies_count += 1
        colored_log(logging.INFO, f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_proxies_count} –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ) –≤ {output_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª: {e}")
    return total_proxies_count


async def load_channel_urls(all_urls_file: str) -> List[str]:
    """Loads channel URLs from the specified file."""
    channel_urls = []
    try:
        with open(all_urls_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url:
                    channel_urls.append(url)
    except FileNotFoundError:
        colored_log(logging.WARNING, f"–§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å URL –∫–∞–Ω–∞–ª–æ–≤.")
        open(all_urls_file, 'w').close() # Create empty file if not exists
    return channel_urls


async def main():
    start_time = time.time()
    channel_urls = await load_channel_urls(ALL_URLS_FILE)
    if not channel_urls:
        colored_log(logging.WARNING, "–ù–µ—Ç URL –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    total_channels = len(channel_urls)
    channels_processed_successfully = 0
    total_proxies_downloaded = 0
    protocol_counts = defaultdict(int)
    channel_status_counts = defaultdict(int)

    resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())
    global_proxy_semaphore = asyncio.Semaphore(MAX_CONCURRENT_PROXIES_GLOBAL)
    channel_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CHANNELS)

    async with aiohttp.ClientSession() as session:
        channel_tasks = []
        for channel_url in channel_urls:
            async def process_channel_task(url):
                channel_proxies_count_channel = 0 # Initialize count here
                channel_success = 0 # Initialize success count
                async with channel_semaphore:
                    colored_log(logging.INFO, f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞: {url}")
                    lines, status = await download_proxies_from_channel(url, session)
                    channel_status_counts[status] += 1
                    if status == "success":
                        parsed_proxies = await parse_and_filter_proxies(lines, resolver)
                        channel_proxies_count_channel = len(parsed_proxies)
                        channel_success = 1 # Mark channel as success after processing
                        for proxy in parsed_proxies:
                            protocol_counts[proxy.protocol] += 1
                        colored_log(logging.INFO, f"‚úÖ –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ù–∞–π–¥–µ–Ω–æ {channel_proxies_count_channel} –ø—Ä–æ–∫—Å–∏.")
                        return channel_proxies_count_channel, channel_success, parsed_proxies # Return counts and proxies
                    else:
                        colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {status}.")
                        return 0, 0, [] # Return zero counts and empty list for failed channels

            task = asyncio.create_task(process_channel_task(channel_url))
            channel_tasks.append(task)

        channel_results = await asyncio.gather(*channel_tasks)
        all_proxies = []
        for proxies_count, success_flag, proxies_list in channel_results: # Unpack returned values
            total_proxies_downloaded += proxies_count # Aggregate proxy counts
            channels_processed_successfully += success_flag # Aggregate success flags
            all_proxies.extend(proxies_list) # Collect proxies

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã) –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
    all_proxies_saved_count = save_all_proxies_to_file(all_proxies, OUTPUT_ALL_CONFIG_FILE)

    unique_proxies_by_protocol = deduplicate_proxies(all_proxies)
    unique_proxies_saved_count = save_proxies_to_file(unique_proxies_by_protocol, OUTPUT_CONFIG_FILE)

    end_time = time.time()
    elapsed_time = end_time - start_time

    colored_log(logging.INFO, "==================== üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–û–ö–°–ò ====================")
    colored_log(logging.INFO, f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {elapsed_time:.2f} —Å–µ–∫")
    colored_log(logging.INFO, f"üîó –í—Å–µ–≥–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_channels}")

    colored_log(logging.INFO, "\nüìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
    for status in ["success", "warning", "error", "critical"]:
        count = channel_status_counts.get(status, 0)
        if count > 0:
            status_text = status.upper()
            color = LogColors.GREEN if status == "success" else (LogColors.YELLOW if status == "warning" else (LogColors.RED if status in ["error", "critical"] else LogColors.RESET))
            colored_log(logging.INFO, f"  - {color}{status_text}{LogColors.RESET}: {count} –∫–∞–Ω–∞–ª–æ–≤")

    colored_log(logging.INFO, f"\n‚ú® –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {total_proxies_downloaded}")
    colored_log(logging.INFO, f"‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {unique_proxies_saved_count} (–≤ {OUTPUT_CONFIG_FILE})")
    colored_log(logging.INFO, f"üìù –í—Å–µ–≥–æ –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {all_proxies_saved_count} (–≤ {OUTPUT_ALL_CONFIG_FILE})")


    colored_log(logging.INFO, "\nüî¨ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º (–Ω–∞–π–¥–µ–Ω–æ):")
    if protocol_counts:
        for protocol, count in protocol_counts.items():
            colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
    else:
        colored_log(logging.INFO, "   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.")

    colored_log(logging.INFO, "======================== üèÅ –ö–û–ù–ï–¶ –°–¢–ê–¢–ò–°–¢–ò–ö–ò =========================")
    colored_log(logging.INFO, "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    asyncio.run(main())
