import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import json
import sys
import argparse
import dataclasses
import random
import aiohttp
import base64
import time
import binascii
import ssl # –î–ª—è TLS-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
from enum import Enum
from urllib.parse import urlparse, parse_qs, urlunparse
from typing import Dict, List, Optional, Tuple, Set, DefaultDict, Any, Union # –î–æ–±–∞–≤–ª–µ–Ω Union
from dataclasses import dataclass, field, asdict
from collections import defaultdict
from string import Template
from functools import lru_cache

# --- –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ ---
try:
    from tqdm.asyncio import tqdm # –î–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤
except ImportError:
    print("Please install tqdm: pip install tqdm")
    sys.exit(1)

try:
    import yaml # –î–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ Clash
except ImportError:
    # –ù–µ –≤—ã—Ö–æ–¥–∏–º, –µ—Å–ª–∏ yaml –Ω–µ –Ω—É–∂–µ–Ω
    yaml = None


# --- Constants ---
LOG_FILE = 'proxy_downloader.log'
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"
LOG_FORMAT: Dict[str, str] = {
    "time": "%(asctime)s",
    "level": "%(levelname)s",
    "message": "%(message)s",
    "process": "%(process)s",
    "threadName": "%(threadName)s",
    "module": "%(module)s",
    "funcName": "%(funcName)s",
    "lineno": "%(lineno)d",
}

DNS_TIMEOUT = 15
HTTP_TIMEOUT = 15
MAX_RETRIES = 4
RETRY_DELAY_BASE = 2
HEADERS = {'User-Agent': 'ProxyDownloader/1.0'}
PROTOCOL_REGEX = re.compile(r"^(vless|tuic|hy2|ss|ssr|trojan)://", re.IGNORECASE)
HOSTNAME_REGEX = re.compile(r"^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$")
PROFILE_NAME_TEMPLATE = Template("${protocol}-${type}-${security}") # –ë–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω

# --- –ù–æ–≤—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ---
TEST_URL = "www.google.com" # URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞, —Ç–æ–ª—å–∫–æ –¥–ª—è TLS SNI)
TEST_PORT = 443 # –ü–æ—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ 443 –¥–ª—è TLS)
TEST_TIMEOUT = 10 # –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
TEST_RESULT_TYPE = Dict[str, Union[str, Optional[float], Optional[str]]] # –¢–∏–ø –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–µ—Å—Ç–∞

COLOR_MAP = {
    logging.INFO: '\033[92m',
    logging.WARNING: '\033[93m',
    logging.ERROR: '\033[91m',
    logging.CRITICAL: '\033[1m\033[91m',
    'RESET': '\033[0m'
}

QUALITY_SCORE_WEIGHTS = {
    "protocol": {"vless": 5, "trojan": 5, "tuic": 4, "hy2": 3, "ss": 2, "ssr": 1},
    "security": {"tls": 3, "none": 0},
    "transport": {"ws": 2, "websocket": 2, "grpc": 2, "tcp": 1, "udp": 0},
}

QUALITY_CATEGORIES = {
    "High": range(8, 15),
    "Medium": range(4, 8),
    "Low": range(0, 4),
}

# --- –ù–æ–≤—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤—ã–≤–æ–¥–∞ ---
class OutputFormat(Enum):
    TEXT = "text"
    JSON = "json"
    CLASH = "clash"
    # V2RAYN = "v2rayn" # –ü–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω —Å–ª–æ–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç

# --- Data Structures ---
class Protocols(Enum):
    VLESS = "vless"
    TUIC = "tuic"
    HY2 = "hy2"
    SS = "ss"
    SSR = "ssr"
    TROJAN = "trojan"

ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]

# --- Logging Setup ---
# (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, tqdm –æ–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å logging)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record: Dict[str, Any] = {}
        for key, format_specifier in LOG_FORMAT.items():
             try: # –î–æ–±–∞–≤–∏–º try-except –Ω–∞ —Å–ª—É—á–∞–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞
                 temp_formatter = logging.Formatter(format_specifier)
                 log_record[key] = temp_formatter.format(record)
             except AttributeError:
                 log_record[key] = None # –ò–ª–∏ –¥—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        log_record["message"] = record.getMessage()
        log_record["level"] = record.levelname
        log_record["time"] = self.formatTime(record, self.default_time_format)
        if record.exc_info:
            log_record['exc_info'] = self.formatException(record.exc_info)
        if hasattr(record, 'taskName') and record.taskName:
             log_record['taskName'] = record.taskName
        return json.dumps(log_record, ensure_ascii=False, default=str)
formatter_file = JsonFormatter()
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)
class ColoredFormatter(logging.Formatter):
    def __init__(self, fmt: str = CONSOLE_LOG_FORMAT, use_colors: bool = True):
        super().__init__(fmt)
        self.use_colors = use_colors
    def format(self, record: logging.LogRecord) -> str:
        message = super().format(record)
        if self.use_colors:
            color_start = COLOR_MAP.get(record.levelno, COLOR_MAP['RESET'])
            color_reset = COLOR_MAP['RESET']
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º print –¥–ª—è –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤, —á—Ç–æ–±—ã tqdm –Ω–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–ª
            # print(f"{color_start}{message}{color_reset}", file=sys.stderr if record.levelno >= logging.WARNING else sys.stdout)
            # return "" # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, —Ç.–∫. —É–∂–µ –Ω–∞–ø–µ—á–∞—Ç–∞–ª–∏
            # --- –ò–õ–ò --- –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, tqdm –¥–æ–ª–∂–µ–Ω —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è
            message = f"{color_start}{message}{color_reset}"
        return message
console_formatter = ColoredFormatter()
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)
def colored_log(level: int, message: str, *args, **kwargs):
    logger.log(level, message, *args, **kwargs)

# --- Data Structures ---
@dataclass(frozen=True)
class ConfigFiles:
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt" # –ë—É–¥–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω–æ —Ñ–æ—Ä–º–∞—Ç–æ–º

@dataclass(frozen=True)
class RetrySettings:
    MAX_RETRIES: int = MAX_RETRIES
    RETRY_DELAY_BASE: int = RETRY_DELAY_BASE

@dataclass(frozen=True)
class ConcurrencyLimits:
    MAX_CHANNELS: int = 60
    MAX_DNS: int = 50 # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ –∏–∑ MAX_PROXIES_GLOBAL
    MAX_TESTS: int = 30 # –ù–æ–≤—ã–π –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–æ–≤

CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()

class ProfileName(Enum):
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    SSR = "SSR"
    TROJAN = "TROJAN"
    UNKNOWN = "Unknown Protocol"

# --- Custom Exceptions ---
class InvalidURLError(ValueError): pass
class UnsupportedProtocolError(ValueError): pass
class EmptyChannelError(Exception): pass
class DownloadError(Exception): pass
class ProxyTestError(Exception): pass # –ù–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ

@dataclass(frozen=True, eq=True)
class ProxyParsedConfig:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    config_string: str
    protocol: str
    address: str
    port: int
    remark: str = ""
    query_params: Dict[str, str] = field(default_factory=dict)
    quality_score: int = 0

    def __hash__(self):
        return hash((self.protocol, self.address, self.port, frozenset(self.query_params.items())))

    def __str__(self):
        return (f"ProxyParsedConfig(protocol={self.protocol}, address={self.address}, "
                f"port={self.port}, config_string='{self.config_string[:50]}...')")

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        original_string = config_string.strip()
        if not original_string: return None
        protocol_match = PROTOCOL_REGEX.match(original_string)
        if not protocol_match: return None
        protocol = protocol_match.group(1).lower()
        try:
            parsed_url = urlparse(original_string)
            if parsed_url.scheme.lower() != protocol: return None
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port: return None
            # if not is_valid_ipv4(address) and not HOSTNAME_REGEX.match(address): return None # –û—Å–ª–∞–±–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –∑–¥–µ—Å—å
            if not 1 <= port <= 65535: return None
            remark = parsed_url.fragment or ""
            query_params_raw = parse_qs(parsed_url.query)
            query_params = {k: v[0] for k, v in query_params_raw.items() if v}
            config_string_to_store = original_string.split('#')[0]
            return cls(
                config_string=config_string_to_store, protocol=protocol, address=address,
                port=port, remark=remark, query_params=query_params,
            )
        except ValueError as e:
            logger.debug(f"URL parsing error for '{original_string[:100]}...': {e}")
            return None
        except Exception as e:
             logger.error(f"Unexpected error parsing URL '{original_string[:100]}...': {e}", exc_info=True)
             return None

# --- Helper Functions ---
@lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    if is_valid_ipv4(hostname): return hostname
    try:
        async with asyncio.timeout(DNS_TIMEOUT):
            logger.debug(f"Attempting DNS query for {hostname}")
            result = await resolver.query(hostname, 'A')
            if result:
                resolved_ip = result[0].host
                if is_valid_ipv4(resolved_ip):
                    logger.debug(f"DNS resolved {hostname} to {resolved_ip}")
                    return resolved_ip
                else:
                    logger.warning(f"DNS resolved {hostname} to non-IPv4 address: {resolved_ip}")
                    return None
            else:
                 logger.debug(f"DNS query for {hostname} returned no results.")
                 return None
    except asyncio.TimeoutError:
        logger.debug(f"DNS resolution timeout for {hostname}")
        return None
    except aiodns.error.DNSError as e:
        error_code = e.args[0] if e.args else "Unknown"
        if error_code == 4: logger.debug(f"DNS resolution error for {hostname}: Host not found (NXDOMAIN)")
        elif error_code == 1: logger.debug(f"DNS resolution error for {hostname}: Format error (FORMERR)")
        else: logger.warning(f"DNS resolution error for {hostname}: {e}, Code: {error_code}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error during DNS resolution for {hostname}: {e}", exc_info=True)
        return None

def assess_proxy_quality(proxy_config: ProxyParsedConfig) -> int:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    score = 0
    protocol = proxy_config.protocol.lower()
    query_params = proxy_config.query_params
    score += QUALITY_SCORE_WEIGHTS["protocol"].get(protocol, 0)
    security = query_params.get("security", "none").lower()
    score += QUALITY_SCORE_WEIGHTS["security"].get(security, 0)
    transport = query_params.get("type", query_params.get("transport", "tcp")).lower()
    score += QUALITY_SCORE_WEIGHTS["transport"].get(transport, 0)
    return score

def get_quality_category(score: int) -> str:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    for category, score_range in QUALITY_CATEGORIES.items():
        if score in score_range:
            return category
    return "Unknown"

def generate_proxy_profile_name(proxy_config: ProxyParsedConfig, test_result: Optional[TEST_RESULT_TYPE] = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –ø—Ä–æ–∫—Å–∏, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è—è –∑–∞–¥–µ—Ä–∂–∫—É.

    Args:
        proxy_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏.
        test_result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å).

    Returns:
        –ò–º—è –ø—Ä–æ—Ñ–∏–ª—è (—Å—Ç—Ä–æ–∫–∞).
    """
    protocol = proxy_config.protocol.upper()
    type_ = proxy_config.query_params.get('type', proxy_config.query_params.get('transport', 'tcp')).lower()
    security = proxy_config.query_params.get('security', 'none').lower()
    quality_category = get_quality_category(proxy_config.quality_score)

    name_parts = {
        "protocol": protocol,
        "type": type_,
        "security": security,
        "quality": f"Q{proxy_config.quality_score}",
        "category": quality_category,
    }

    # –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è
    base_name = f"{protocol}-{type_}-{security}-Q{proxy_config.quality_score}-{quality_category}"

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É, –µ—Å–ª–∏ —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω
    if test_result and test_result.get('status') == 'ok' and test_result.get('latency') is not None:
        latency_ms = int(test_result['latency'] * 1000)
        base_name += f"-{latency_ms}ms"

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π remark, –µ—Å–ª–∏ –æ–Ω –±—ã–ª
    if proxy_config.remark:
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ remark
        safe_remark = re.sub(r'[#\s]+', '_', proxy_config.remark)
        base_name += f"_{safe_remark}"

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    max_len = 60
    if len(base_name) > max_len:
        base_name = base_name[:max_len-3] + "..."

    return base_name


# --- Core Logic Functions ---

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> List[str]:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=HTTP_TIMEOUT)
    while retries_attempted <= RETRY.MAX_RETRIES:
        try:
            logger.debug(f"Attempting download from {channel_url} (Attempt {retries_attempted + 1})")
            async with session.get(channel_url, timeout=session_timeout, headers=HEADERS) as response:
                response.raise_for_status()
                logger.debug(f"Successfully connected to {channel_url}, status: {response.status}")
                content_bytes = await response.read()
                if not content_bytes.strip():
                    logger.warning(f"Channel {channel_url} returned empty or whitespace-only response.")
                    raise EmptyChannelError(f"Channel {channel_url} returned empty response.")
                decoded_text: Optional[str] = None
                decode_method: str = "Unknown"
                try: # –ü–æ–ø—ã—Ç–∫–∞ Base64
                    base64_bytes_stripped = bytes("".join(content_bytes.decode('latin-1').split()), 'latin-1')
                    missing_padding = len(base64_bytes_stripped) % 4
                    if missing_padding: base64_bytes_padded = base64_bytes_stripped + b'=' * (4 - missing_padding)
                    else: base64_bytes_padded = base64_bytes_stripped
                    b64_decoded_bytes = base64.b64decode(base64_bytes_padded, validate=True)
                    decoded_text_from_b64 = b64_decoded_bytes.decode('utf-8')
                    if PROTOCOL_REGEX.search(decoded_text_from_b64):
                        logger.debug(f"Content from {channel_url} successfully decoded as Base64.")
                        decoded_text = decoded_text_from_b64
                        decode_method = "Base64"
                    else:
                        logger.debug(f"Content from {channel_url} decoded from Base64, but no protocol found. Trying plain text.")
                except (binascii.Error, ValueError) as e: logger.debug(f"Content from {channel_url} is not valid Base64 ({type(e).__name__}). Treating as plain text.")
                except UnicodeDecodeError as e: logger.warning(f"Content from {channel_url} decoded from Base64, but result is not valid UTF-8: {e}. Treating as plain text.")
                except Exception as e: logger.error(f"Unexpected error during Base64 processing for {channel_url}: {e}", exc_info=True)

                if decoded_text is None: # –ü–æ–ø—ã—Ç–∫–∞ Plain Text
                    try:
                        logger.debug(f"Attempting to decode content from {channel_url} as plain UTF-8 text.")
                        decoded_text = content_bytes.decode('utf-8')
                        decode_method = "Plain UTF-8"
                    except UnicodeDecodeError:
                        logger.warning(f"UTF-8 decoding failed for {channel_url} (plain text), replacing errors.")
                        decoded_text = content_bytes.decode('utf-8', errors='replace')
                        decode_method = "Plain UTF-8 (with replace)"

                if decoded_text is not None:
                    logger.info(f"Successfully decoded content from {channel_url} using method: {decode_method}")
                    return decoded_text.splitlines()
                else:
                    logger.error(f"Failed to decode content from {channel_url} using any method.")
                    raise DownloadError(f"Failed to decode content from {channel_url}")
        except aiohttp.ClientResponseError as e:
            colored_log(logging.WARNING, f"‚ö†Ô∏è Channel {channel_url} returned HTTP error {e.status}: {e.message}")
            logger.debug(f"Response headers for {channel_url} on error: {response.headers}")
            raise DownloadError(f"HTTP error {e.status} for {channel_url}") from e
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted) + random.uniform(-0.5, 0.5)
            retry_delay = max(0.5, retry_delay)
            colored_log(logging.WARNING, f"‚ö†Ô∏è Error getting {channel_url} (attempt {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {type(e).__name__}. Retry in {retry_delay:.2f}s...")
            if retries_attempted == RETRY.MAX_RETRIES:
                colored_log(logging.ERROR, f"‚ùå Max retries ({RETRY.MAX_RETRIES+1}) reached for {channel_url}")
                raise DownloadError(f"Max retries reached for {channel_url}") from e
            await asyncio.sleep(retry_delay)
        except EmptyChannelError as e: raise e
        except Exception as e:
             logger.error(f"Unexpected error downloading/processing {channel_url}: {e}", exc_info=True)
             raise DownloadError(f"Unexpected error downloading/processing {channel_url}") from e
        retries_attempted += 1
    logger.critical(f"Download loop finished unexpectedly for {channel_url}")
    raise DownloadError(f"Download failed unexpectedly after retries for {channel_url}")

def parse_proxy_lines(lines: List[str]) -> Tuple[List[ProxyParsedConfig], int, int]:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    parsed_configs: List[ProxyParsedConfig] = []
    processed_configs_hashes: Set[int] = set()
    invalid_url_count = 0
    duplicate_count = 0
    for line_num, line in enumerate(lines, 1):
        line = line.strip()
        if not line or line.startswith('#'): continue
        parsed_config = ProxyParsedConfig.from_url(line)
        if parsed_config is None:
            # logger.debug(f"Line {line_num}: Invalid proxy format skipped: {line[:100]}...") # –£–∂–µ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –≤ from_url
            invalid_url_count += 1
            continue
        config_hash = hash(parsed_config)
        if config_hash in processed_configs_hashes:
            logger.debug(f"Line {line_num}: Skipping duplicate proxy (parsed): {parsed_config}")
            duplicate_count += 1
            continue
        processed_configs_hashes.add(config_hash)
        parsed_configs.append(parsed_config)
    logger.info(f"Initial parsing: {len(parsed_configs)} potentially valid configs found. "
                f"Skipped {invalid_url_count} invalid lines, {duplicate_count} duplicates (parsed).")
    return parsed_configs, invalid_url_count, duplicate_count

async def resolve_and_assess_proxies(
    configs: List[ProxyParsedConfig], resolver: aiodns.DNSResolver
) -> Tuple[List[ProxyParsedConfig], int]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –∞–¥—Ä–µ—Å–∞ –ø—Ä–æ–∫—Å–∏ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö –∫–∞—á–µ—Å—Ç–≤–æ.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç `tqdm` –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ DNS-—Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞.
    (–û—Å—Ç–∞–ª—å–Ω–æ–π –¥–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    """
    resolved_configs_with_score: List[ProxyParsedConfig] = []
    dns_resolution_failed_count = 0
    final_unique_keys: Set[tuple] = set()
    dns_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_DNS)

    async def resolve_task(config: ProxyParsedConfig) -> Optional[ProxyParsedConfig]:
        nonlocal dns_resolution_failed_count
        async with dns_semaphore:
            resolved_ip = await resolve_address(config.address, resolver)
        if resolved_ip:
            quality_score = assess_proxy_quality(config)
            final_key = (config.protocol, resolved_ip, config.port, frozenset(config.query_params.items()))
            if final_key not in final_unique_keys:
                final_unique_keys.add(final_key)
                return dataclasses.replace(config, quality_score=quality_score)
            else:
                logger.debug(f"Skipping duplicate proxy after DNS resolution: {config.address} -> {resolved_ip} (Port: {config.port}, Proto: {config.protocol})")
                return None
        else:
            dns_resolution_failed_count += 1
            return None

    tasks = [resolve_task(cfg) for cfg in configs]
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm.gather –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    results = await tqdm.gather(*tasks, desc="Resolving DNS", unit="proxy", disable=not sys.stdout.isatty())

    resolved_configs_with_score = [res for res in results if res is not None]
    logger.info(f"DNS Resolution & Assessment: {len(resolved_configs_with_score)} unique configs resolved and assessed. "
                f"{dns_resolution_failed_count} DNS resolution failures or post-resolution duplicates.")
    return resolved_configs_with_score, dns_resolution_failed_count

# --- –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∫—Å–∏ ---
async def test_proxy_connectivity(proxy_config: ProxyParsedConfig) -> TEST_RESULT_TYPE:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Ö–æ—Å—Ç–æ–º:–ø–æ—Ä—Ç–æ–º –ø—Ä–æ–∫—Å–∏.

    –ü—ã—Ç–∞–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å TCP-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏, –µ—Å–ª–∏ security=tls, –≤—ã–ø–æ–ª–Ω—è–µ—Ç TLS handshake.
    –ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.
    **–í–ù–ò–ú–ê–ù–ò–ï:** –≠—Ç–æ –ù–ï –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –ø—Ä–æ–∫—Å–∏ (VLESS/Trojan –∏ —Ç.–¥.).

    Args:
        proxy_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–µ—Å—Ç–∞.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {'status': 'ok'/'failed', 'latency': float/None, 'error': str/None}
    """
    start_time = time.monotonic()
    writer = None
    reader = None
    host = proxy_config.address # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–¥—Ä–µ—Å (–º–æ–∂–µ—Ç –±—ã—Ç—å IP –∏–ª–∏ hostname)
    port = proxy_config.port
    use_tls = proxy_config.query_params.get('security', 'none').lower() == 'tls'

    try:
        logger.debug(f"Testing connection to {host}:{port} (TLS: {use_tls})")
        async with asyncio.timeout(TEST_TIMEOUT):
            reader, writer = await asyncio.open_connection(host, port)

            if use_tls:
                logger.debug(f"Attempting TLS handshake with {host}:{port}")
                ssl_context = ssl.create_default_context()
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥—Ä–µ—Å –∫–∞–∫ server_hostname –¥–ª—è SNI, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ IP
                server_hostname = host if not is_valid_ipv4(host) else None
                transport = writer.get_extra_info('transport')
                if not transport:
                     raise ProxyTestError("Could not get transport info for TLS")

                # –ó–∞–ø—É—Å–∫–∞–µ–º TLS handshake
                # –í –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö asyncio/Python —ç—Ç–æ –º–æ–∂–µ—Ç –¥–µ–ª–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ start_tls
                # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º wrap_socket (–º–æ–∂–µ—Ç –±—ã—Ç—å –±–ª–æ–∫–∏—Ä—É—é—â–∏–º!)
                # –ü—Ä–∞–≤–∏–ª—å–Ω–µ–µ –±—ã–ª–æ –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π handshake, –Ω–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–µ–µ.
                # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞!
                loop = asyncio.get_running_loop()
                # –í—ã–ø–æ–ª–Ω—è–µ–º wrap_socket –≤ executor, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫
                # –≠—Ç–æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å, –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π TLS handshake —Å–ª–æ–∂–Ω–µ–µ
                try:
                    new_transport = await loop.start_tls(transport, ssl_context, server_hostname=server_hostname)
                    # –û–±–Ω–æ–≤–ª—è–µ–º reader/writer, –µ—Å–ª–∏ start_tls –≤–µ—Ä–Ω—É–ª –Ω–æ–≤—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
                    # (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ asyncio)
                    # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–Ω–∞—Ç—å, —á—Ç–æ handshake –ø—Ä–æ—à–µ–ª –±–µ–∑ –æ—à–∏–±–æ–∫
                    logger.debug(f"TLS handshake successful for {host}:{port}")
                except ssl.SSLError as tls_err:
                    raise ProxyTestError(f"TLS handshake failed: {tls_err}") from tls_err
                except Exception as handshake_err: # –õ–æ–≤–∏–º –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ start_tls
                    raise ProxyTestError(f"TLS start_tls error: {handshake_err}") from handshake_err


            latency = time.monotonic() - start_time
            logger.debug(f"Connection test OK for {host}:{port}, latency: {latency:.4f}s")
            return {'status': 'ok', 'latency': latency, 'error': None}

    except asyncio.TimeoutError:
        logger.debug(f"Connection test TIMEOUT for {host}:{port}")
        return {'status': 'failed', 'latency': None, 'error': 'Timeout'}
    except (OSError, ConnectionRefusedError, ProxyTestError, ssl.SSLError, Exception) as e:
        logger.debug(f"Connection test FAILED for {host}:{port}: {type(e).__name__}: {e}")
        return {'status': 'failed', 'latency': None, 'error': f"{type(e).__name__}: {str(e)[:100]}"}
    finally:
        if writer:
            try:
                writer.close()
                await writer.wait_closed()
            except Exception:
                pass # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏

async def run_proxy_tests(
    proxies: List[ProxyParsedConfig]
) -> List[Tuple[ProxyParsedConfig, TEST_RESULT_TYPE]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∫—Å–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ `tqdm` –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.

    Args:
        proxies: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: [(ProxyParsedConfig, test_result_dict), ...]
    """
    if not proxies:
        return []

    test_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_TESTS)
    results_with_proxies: List[Tuple[ProxyParsedConfig, TEST_RESULT_TYPE]] = []

    async def test_task_wrapper(proxy: ProxyParsedConfig) -> Tuple[ProxyParsedConfig, TEST_RESULT_TYPE]:
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∞ —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º."""
        async with test_semaphore:
            result = await test_proxy_connectivity(proxy)
        return proxy, result

    tasks = [test_task_wrapper(p) for p in proxies]
    results_with_proxies = await tqdm.gather(*tasks, desc="Testing Proxies", unit="proxy", disable=not sys.stdout.isatty())

    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Å—Ç–æ–≤
    ok_count = sum(1 for _, res in results_with_proxies if res['status'] == 'ok')
    failed_count = len(results_with_proxies) - ok_count
    logger.info(f"Proxy Connectivity Test Results: {ok_count} OK, {failed_count} Failed.")

    return results_with_proxies


# --- –§—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö ---

def _save_as_text(proxies_with_results: List[Tuple[ProxyParsedConfig, Optional[TEST_RESULT_TYPE]]], file_path: str) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (URL#remark)."""
    count = 0
    lines_to_write = []
    for proxy_conf, test_result in proxies_with_results:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è, –≤–∫–ª—é—á–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        profile_name = generate_proxy_profile_name(proxy_conf, test_result)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º config_string (URL –±–µ–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ fragment)
        config_line = f"{proxy_conf.config_string}#{profile_name}\n"
        lines_to_write.append(config_line)
        count += 1

    with open(file_path, 'w', encoding='utf-8') as f:
        f.writelines(lines_to_write)
    return count

def _save_as_json(proxies_with_results: List[Tuple[ProxyParsedConfig, Optional[TEST_RESULT_TYPE]]], file_path: str) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–ø–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤."""
    count = 0
    output_list = []
    for proxy_conf, test_result in proxies_with_results:
        proxy_dict = asdict(proxy_conf)
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if test_result:
            proxy_dict['test_status'] = test_result.get('status')
            proxy_dict['latency_sec'] = test_result.get('latency')
            proxy_dict['test_error'] = test_result.get('error')
        else:
            proxy_dict['test_status'] = None
            proxy_dict['latency_sec'] = None
            proxy_dict['test_error'] = None
        output_list.append(proxy_dict)
        count += 1

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(output_list, f, indent=2, ensure_ascii=False)
    return count

def _proxy_to_clash_dict(proxy_conf: ProxyParsedConfig, test_result: Optional[TEST_RESULT_TYPE]) -> Optional[Dict[str, Any]]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç ProxyParsedConfig –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è Clash YAML."""
    clash_proxy: Dict[str, Any] = {}
    params = proxy_conf.query_params
    protocol = proxy_conf.protocol.lower()

    # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
    clash_proxy['name'] = generate_proxy_profile_name(proxy_conf, test_result)
    clash_proxy['server'] = proxy_conf.address
    clash_proxy['port'] = proxy_conf.port

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–ª—è Clash
    if protocol == 'vless':
        clash_proxy['type'] = 'vless'
        clash_proxy['uuid'] = proxy_conf.config_string.split('://')[1].split('@')[0] # –ò–∑–≤–ª–µ–∫–∞–µ–º UUID
        clash_proxy['tls'] = params.get('security', 'none') == 'tls'
        clash_proxy['network'] = params.get('type', 'tcp') # ws, grpc, tcp
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã VLESS
        if 'flow' in params: clash_proxy['flow'] = params['flow']
        if 'sni' in params: clash_proxy['servername'] = params['sni']
        if clash_proxy['network'] == 'ws':
            clash_proxy['ws-opts'] = {'path': params.get('path', '/'), 'headers': {'Host': params.get('host', proxy_conf.address)}}
        elif clash_proxy['network'] == 'grpc':
            clash_proxy['grpc-opts'] = {'grpc-service-name': params.get('serviceName', '')}
        # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã vless ...
    elif protocol == 'trojan':
        clash_proxy['type'] = 'trojan'
        clash_proxy['password'] = proxy_conf.config_string.split('://')[1].split('@')[0] # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–æ–ª—å
        clash_proxy['tls'] = params.get('security', 'none') == 'tls' # Trojan –æ–±—ã—á–Ω–æ —Å TLS
        if 'sni' in params: clash_proxy['sni'] = params['sni']
        if 'allowInsecure' in params: clash_proxy['skip-cert-verify'] = params['allowInsecure'].lower() == 'true'
        network = params.get('type', 'tcp')
        if network == 'ws':
             clash_proxy['network'] = 'ws'
             clash_proxy['ws-opts'] = {'path': params.get('path', '/'), 'headers': {'Host': params.get('host', proxy_conf.address)}}
        elif network == 'grpc':
             clash_proxy['network'] = 'grpc'
             clash_proxy['grpc-opts'] = {'grpc-service-name': params.get('serviceName', '')}
        # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã trojan ...
    elif protocol == 'ss':
        clash_proxy['type'] = 'ss'
        # –ü–∞—Ä—Å–∏–Ω–≥ SS URL (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–∑-–∑–∞ base64 —á–∞—Å—Ç–∏)
        try:
            user_info, server_info = proxy_conf.config_string.split('://')[1].split('@')
            server_part = server_info.split('#')[0] # –£–±–∏—Ä–∞–µ–º remark –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ —Å—Ç—Ä–æ–∫–µ
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º user_info (method:password)
            decoded_user = base64.urlsafe_b64decode(user_info + '===').decode('utf-8') # –î–æ–±–∞–≤–ª—è–µ–º padding
            clash_proxy['cipher'], clash_proxy['password'] = decoded_user.split(':', 1)
        except Exception as e:
            logger.warning(f"Could not parse SS URL for Clash: {proxy_conf.config_string} - {e}")
            return None # –ù–µ –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥
        # ... –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ss (plugin, etc.) ...
    # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É TUIC, HY2, SSR –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–ø–æ—Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞–Ω–∏—è –∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ Clash)
    else:
        logger.debug(f"Protocol {protocol} not currently supported for Clash output format.")
        return None # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã

    return clash_proxy

def _save_as_clash(proxies_with_results: List[Tuple[ProxyParsedConfig, Optional[TEST_RESULT_TYPE]]], file_path: str) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Clash YAML."""
    if not yaml:
        logger.error("PyYAML is not installed. Cannot save in Clash format. Please install: pip install pyyaml")
        return 0

    count = 0
    clash_proxies_list = []
    for proxy_conf, test_result in proxies_with_results:
        clash_dict = _proxy_to_clash_dict(proxy_conf, test_result)
        if clash_dict:
            clash_proxies_list.append(clash_dict)
            count += 1

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É Clash config
    clash_config = {
        'proxies': clash_proxies_list,
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–∞–∑–æ–≤—ã–µ proxy-groups, rules –∏ —Ç.–¥.
        'proxy-groups': [
            {'name': 'PROXY', 'type': 'select', 'proxies': [p['name'] for p in clash_proxies_list] + ['DIRECT']}
        ],
        'rules': [
            'MATCH,PROXY'
        ]
    }

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(clash_config, f, allow_unicode=True, sort_keys=False, default_flow_style=None)
    except Exception as e:
        logger.error(f"Error writing Clash YAML file: {e}", exc_info=True)
        return 0 # –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏
    return count

def save_proxies(
    proxies_with_results: List[Tuple[ProxyParsedConfig, Optional[TEST_RESULT_TYPE]]],
    output_file_base: str,
    output_format: OutputFormat
) -> int:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

    Args:
        proxies_with_results: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø—Ä–æ–∫—Å–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç_—Ç–µ—Å—Ç–∞).
                              –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å None, –µ—Å–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å.
        output_file_base: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è).
        output_format: –§–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (enum OutputFormat).

    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏.
    """
    if not proxies_with_results:
        logger.warning("No proxies to save.")
        return 0

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if output_format == OutputFormat.JSON:
        file_path = f"{output_file_base}.json"
        save_func = _save_as_json
    elif output_format == OutputFormat.CLASH:
        file_path = f"{output_file_base}.yaml"
        save_func = _save_as_clash
    else: # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é TEXT
        file_path = f"{output_file_base}.txt"
        save_func = _save_as_text

    saved_count = 0
    try:
        os.makedirs(os.path.dirname(file_path) or '.', exist_ok=True)
        logger.info(f"Attempting to save {len(proxies_with_results)} proxies to {file_path} (Format: {output_format.value})")
        saved_count = save_func(proxies_with_results, file_path)
        if saved_count > 0:
            logger.info(f"Successfully wrote {saved_count} proxies to {file_path}")
        else:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ save_func
             logger.warning(f"No proxies were written to {file_path}")

    except IOError as e:
        logger.error(f"IOError saving proxies to file '{file_path}': {e}", exc_info=True)
        return 0
    except Exception as e:
        logger.error(f"Unexpected error saving proxies to file '{file_path}': {e}", exc_info=True)
        return 0
    return saved_count


# --- –ó–∞–≥—Ä—É–∑–∫–∞ URL –∫–∞–Ω–∞–ª–æ–≤ ---
async def load_channel_urls(all_urls_file: str) -> List[str]:
    """(–î–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    channel_urls: List[str] = []
    try:
        with open(all_urls_file, 'r', encoding='utf-8-sig') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'): channel_urls.append(url)
        logger.info(f"Loaded {len(channel_urls)} channel URLs from {all_urls_file}")
    except FileNotFoundError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è File {all_urls_file} not found. Creating an empty file.")
        try:
            os.makedirs(os.path.dirname(all_urls_file) or '.', exist_ok=True)
            open(all_urls_file, 'w').close()
        except Exception as e: logger.error(f"Error creating file {all_urls_file}: {e}", exc_info=True)
    except Exception as e: logger.error(f"Error opening/reading file {all_urls_file}: {e}", exc_info=True)
    return channel_urls


# --- –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ ---
async def process_channel_task(channel_url: str, session: aiohttp.ClientSession,
                              resolver: aiodns.DNSResolver
                              ) -> List[ProxyParsedConfig]:
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –ø–∞—Ä—Å–∏–Ω–≥, —Ä–µ–∑–æ–ª–≤–∏–Ω–≥, –æ—Ü–µ–Ω–∫–∞.
    (–û—Å—Ç–∞–ª—å–Ω–æ–π –¥–æ–∫—Å—Ç—Ä–∏–Ω–≥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    """
    # colored_log(logging.INFO, f"üöÄ Processing channel: {channel_url}") # –£–±—Ä–∞–Ω–æ, —Ç.–∫. –µ—Å—Ç—å tqdm
    try:
        lines = await download_proxies_from_channel(channel_url, session)
        if not lines: return []
        parsed_proxies_basic, _, _ = parse_proxy_lines(lines)
        if not parsed_proxies_basic: return []
        # –†–µ–∑–æ–ª–≤–∏–Ω–≥ —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏
        resolved_proxies, _ = await resolve_and_assess_proxies(parsed_proxies_basic, resolver)
        channel_proxies_count = len(resolved_proxies)
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–Ω–∞–ª–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
        # logger.info(f"Channel {channel_url} processed. Found {channel_proxies_count} potentially valid proxies after DNS.")
        return resolved_proxies
    except EmptyChannelError:
         logger.warning(f"Channel {channel_url} was empty.")
         return []
    except DownloadError as e:
         logger.error(f"Failed to process channel {channel_url} due to download/decode error: {e}")
         return []
    except Exception as e:
         logger.error(f"Unexpected error processing channel {channel_url}: {e}", exc_info=True)
         return []


# --- –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤ ---
async def load_and_process_channels(channel_urls: List[str], session: aiohttp.ClientSession,
                                     resolver: aiodns.DNSResolver
                                     ) -> Tuple[int, int, List[ProxyParsedConfig], DefaultDict[str, int]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∫–∞–Ω–∞–ª–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç `tqdm` –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤.
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏.

    Args:
        (–ê—Ä–≥—É–º–µ–Ω—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

    Returns:
        Tuple: –ö–æ—Ä—Ç–µ–∂ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - total_proxies_found_before_final_dedup (int)
            - channels_processed_count (int)
            - all_unique_proxies (List[ProxyParsedConfig]): –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
              *–ø–æ—Å–ª–µ* —Ä–µ–∑–æ–ª–≤–∏–Ω–≥–∞, –Ω–æ *–¥–æ* —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
            - channel_status_counts (DefaultDict[str, int])
    """
    channels_processed_count = 0
    total_proxies_found_before_final_dedup = 0
    channel_status_counts: DefaultDict[str, int] = defaultdict(int)
    channel_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_CHANNELS)
    all_proxies_from_channels: List[ProxyParsedConfig] = [] # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–∫—Å–∏ —Å—é–¥–∞

    async def task_wrapper(url: str) -> Optional[List[ProxyParsedConfig]]:
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        nonlocal channels_processed_count
        async with channel_semaphore:
            try:
                result = await process_channel_task(url, session, resolver)
                channels_processed_count += 1
                return result # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º)
            except Exception as e:
                logger.error(f"Critical task failure wrapper for {url}: {e}", exc_info=True)
                channels_processed_count += 1
                channel_status_counts["critical_wrapper_error"] += 1
                return None # –û—à–∏–±–∫–∞ –≤ —Å–∞–º–æ–π –æ–±–µ—Ä—Ç–∫–µ

    tasks = [asyncio.create_task(task_wrapper(channel_url)) for channel_url in channel_urls]
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm.gather –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    channel_results = await tqdm.gather(*tasks, desc="Processing channels", unit="channel", disable=not sys.stdout.isatty())

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    unique_proxies_set: Set[ProxyParsedConfig] = set()
    for result in channel_results:
        if result is None: # –û—à–∏–±–∫–∞ –≤ –æ–±–µ—Ä—Ç–∫–µ
            continue
        elif isinstance(result, list):
            proxies_from_channel = result
            unique_proxies_set.update(proxies_from_channel)
            if proxies_from_channel:
                channel_status_counts["success_found_proxies"] += 1
                total_proxies_found_before_final_dedup += len(proxies_from_channel)
            else:
                channel_status_counts["success_no_proxies"] += 1
        else:
             logger.warning(f"Unexpected result type from channel gather: {type(result)}")
             channel_status_counts["unknown_error"] += 1

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º set –≤ —Å–ø–∏—Å–æ–∫ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –±—É–¥–µ—Ç –ø–æ–∑–∂–µ, –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤)
    all_unique_proxies: List[ProxyParsedConfig] = list(unique_proxies_set)
    final_unique_count = len(all_unique_proxies)
    logger.info(f"Total unique proxies found after DNS/deduplication: {final_unique_count}")

    return (total_proxies_found_before_final_dedup,
            channels_processed_count,
            all_unique_proxies, # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ —Ç–µ—Å—Ç–æ–≤
            channel_status_counts)


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
def output_statistics(start_time: float, total_channels_requested: int, channels_processed_count: int,
                      channel_status_counts: DefaultDict[str, int], total_proxies_found_before_dedup: int,
                      proxies_after_dns_count: int,
                      proxies_after_test_count: Optional[int], # –ú–æ–∂–µ—Ç –±—ã—Ç—å None
                      all_proxies_saved_count: int,
                      protocol_counts: DefaultDict[str, int], # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º
                      quality_category_counts: DefaultDict[str, int], # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º
                      output_file_path: str, # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                      output_format: OutputFormat):
    """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª—å."""
    end_time = time.time()
    elapsed_time = end_time - start_time
    colored_log(logging.INFO, "==================== üìä PROXY DOWNLOAD STATISTICS ====================")
    colored_log(logging.INFO, f"‚è±Ô∏è  Script runtime: {elapsed_time:.2f} seconds")
    colored_log(logging.INFO, f"üîó Total channel URLs requested: {total_channels_requested}")
    colored_log(logging.INFO, f"üõ†Ô∏è Total channels processed (attempted): {channels_processed_count}/{total_channels_requested}")

    colored_log(logging.INFO, "\nüìä Channel Processing Status:")
    status_order = ["success_found_proxies", "success_no_proxies", "critical_wrapper_error", "unknown_error"]
    status_colors = {"success_found_proxies": '\033[92m', "success_no_proxies": '\033[93m', "critical_wrapper_error": '\033[91m', "unknown_error": '\033[91m'}
    status_texts = {"success_found_proxies": "SUCCESS (found proxies)", "success_no_proxies": "SUCCESS (0 valid proxies found)", "critical_wrapper_error": "CRITICAL TASK ERROR", "unknown_error": "UNKNOWN ERROR"}
    processed_keys = set()
    for status_key in status_order:
        if status_key in channel_status_counts:
            count = channel_status_counts[status_key]
            color_start = status_colors.get(status_key, COLOR_MAP['RESET'])
            status_text = status_texts.get(status_key, status_key.upper())
            colored_log(logging.INFO, f"  - {color_start}{status_text}{COLOR_MAP['RESET']}: {count} channels")
            processed_keys.add(status_key)
    for status_key, count in channel_status_counts.items():
         if status_key not in processed_keys:
             color_start = status_colors.get(status_key, COLOR_MAP['RESET'])
             status_text = status_texts.get(status_key, status_key.replace('_', ' ').upper())
             colored_log(logging.INFO, f"  - {color_start}{status_text}{COLOR_MAP['RESET']}: {count} channels")

    colored_log(logging.INFO, f"\n‚ú® Proxies found (before final deduplication): {total_proxies_found_before_dedup}")
    colored_log(logging.INFO, f"üß¨ Proxies after DNS resolution & deduplication: {proxies_after_dns_count}")
    if proxies_after_test_count is not None:
        colored_log(logging.INFO, f"‚úÖ Proxies passed connectivity test: {proxies_after_test_count}")
    colored_log(logging.INFO, f"üìù Total proxies saved: {all_proxies_saved_count} (to {output_file_path}, format: {output_format.value})")

    colored_log(logging.INFO, "\nüî¨ Protocol Breakdown (saved proxies):")
    if protocol_counts:
        for protocol, count in sorted(protocol_counts.items()):
            colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
    else:
        colored_log(logging.INFO, "   No protocol statistics available for saved proxies.")

    colored_log(logging.INFO, "\n‚≠êÔ∏è Proxy Quality Category Distribution (saved proxies):")
    if quality_category_counts:
         category_order = {"High": 0, "Medium": 1, "Low": 2, "Unknown": 3}
         for category, count in sorted(quality_category_counts.items(), key=lambda item: category_order.get(item[0], 99)):
             colored_log(logging.INFO, f"   - {category}: {count} proxies")
    else:
        colored_log(logging.INFO, "   No quality category statistics available for saved proxies.")
    colored_log(logging.INFO, "======================== üèÅ STATISTICS END =========================")


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è main —Ñ—É–Ω–∫—Ü–∏—è ---
async def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(description="Proxy Downloader Script")
    parser.add_argument('--nocolorlogs', action='store_true', help='Disable colored console logs')
    parser.add_argument('--test-proxies', action='store_true', help='Enable basic connectivity test for proxies')
    parser.add_argument(
        '--output-format',
        type=str,
        choices=[f.value for f in OutputFormat],
        default=OutputFormat.TEXT.value,
        help=f'Output file format (default: {OutputFormat.TEXT.value})'
    )
    parser.add_argument(
        '--input', '-i',
        type=str,
        default=CONFIG_FILES.ALL_URLS,
        help=f'Input file with channel URLs (default: {CONFIG_FILES.ALL_URLS})'
     )
    parser.add_argument(
         '--output', '-o',
         type=str,
         default=CONFIG_FILES.OUTPUT_ALL_CONFIG,
         help=f'Output file path base (without extension) (default: {CONFIG_FILES.OUTPUT_ALL_CONFIG})'
     )

    args = parser.parse_args()

    console_formatter.use_colors = not args.nocolorlogs
    output_format_enum = OutputFormat(args.output_format)
    input_file = args.input
    output_file_base = args.output

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–æ–≤
    if output_format_enum == OutputFormat.CLASH and not yaml:
         colored_log(logging.ERROR, "‚ùå PyYAML is required for Clash output format. Please install: pip install pyyaml")
         sys.exit(1)

    try:
        start_time = time.time()
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ URL –∫–∞–Ω–∞–ª–æ–≤
        channel_urls = await load_channel_urls(input_file)
        total_channels_requested = len(channel_urls)
        if not channel_urls:
            colored_log(logging.WARNING, "No channel URLs found in the input file. Exiting.")
            return

        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())
        async with aiohttp.ClientSession() as session:
            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–æ–≤ (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –ø–∞—Ä—Å–∏–Ω–≥, DNS)
            (total_proxies_found_before_dedup, channels_processed_count,
             proxies_after_dns, # –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –ø–æ—Å–ª–µ DNS
             channel_status_counts) = await load_and_process_channels(
                channel_urls, session, resolver)

        proxies_after_dns_count = len(proxies_after_dns)
        proxies_to_save_with_results: List[Tuple[ProxyParsedConfig, Optional[TEST_RESULT_TYPE]]] = []
        proxies_after_test_count: Optional[int] = None # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

        # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if args.test_proxies:
            test_results_with_proxies = await run_proxy_tests(proxies_after_dns)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏
            working_proxies_with_results = [
                (proxy, result) for proxy, result in test_results_with_proxies if result['status'] == 'ok'
            ]
            proxies_after_test_count = len(working_proxies_with_results)
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –ø–æ –∑–∞–¥–µ—Ä–∂–∫–µ (–≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ)
            working_proxies_with_results.sort(key=lambda item: item[1]['latency'] or float('inf'))
            proxies_to_save_with_results = working_proxies_with_results
            logger.info(f"Filtered proxies after testing. Kept {proxies_after_test_count} working proxies.")
        else:
            # –ï—Å–ª–∏ —Ç–µ—Å—Ç—ã –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª–∏—Å—å, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –≥–æ—Ç–æ–≤–∏–º –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é
            proxies_after_dns.sort(key=lambda p: p.quality_score, reverse=True)
            proxies_to_save_with_results = [(proxy, None) for proxy in proxies_after_dns] # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞ None
            logger.info("Skipping proxy connectivity tests.")


        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–æ—Ä–º–∞—Ç–∞
        if output_format_enum == OutputFormat.JSON: file_ext = ".json"
        elif output_format_enum == OutputFormat.CLASH: file_ext = ".yaml"
        else: file_ext = ".txt"
        output_file_path = output_file_base + file_ext

        all_proxies_saved_count = save_proxies(
            proxies_to_save_with_results,
            output_file_base, # –ü–µ—Ä–µ–¥–∞–µ–º –±–∞–∑—É –∏–º–µ–Ω–∏
            output_format_enum
        )

        # 6. –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
        saved_protocol_counts: DefaultDict[str, int] = defaultdict(int)
        saved_quality_category_counts: DefaultDict[str, int] = defaultdict(int)
        for proxy, _ in proxies_to_save_with_results: # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ
             if all_proxies_saved_count > 0: # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
                 saved_protocol_counts[proxy.protocol] += 1
                 quality_category = get_quality_category(proxy.quality_score)
                 saved_quality_category_counts[quality_category] += 1


        # 7. –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        output_statistics(start_time, total_channels_requested, channels_processed_count,
                          channel_status_counts, total_proxies_found_before_dedup,
                          proxies_after_dns_count, proxies_after_test_count,
                          all_proxies_saved_count, saved_protocol_counts,
                          saved_quality_category_counts, output_file_path, # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
                          output_format_enum)

    except Exception as e:
        logger.critical(f"Unexpected critical error in main execution: {e}", exc_info=True)
        sys.exit(1)
    finally:
        colored_log(logging.INFO, "‚úÖ Proxy download and processing script finished.")


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ —Ü–∏–∫–ª–∞ —Å–æ–±—ã—Ç–∏–π –¥–ª—è Windows, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–¥–ª—è ProactorEventLoop)
    # if sys.platform == 'win32':
    #     asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    asyncio.run(main())
