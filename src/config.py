import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import io
import uuid
import string
import base64
import aiohttp
import time
import json
import functools
import inspect
import sys
import argparse
import dataclasses

from enum import Enum
from urllib.parse import urlparse, parse_qs, urlsplit
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
LOG_FORMAT = {
    "time": "%(asctime)s",
    "level": "%(levelname)s",
    "message": "%(message)s",
    "process": "%(process)s",
    "module": "%(module)s",
    "funcName": "%(funcName)s",
    "lineno": "%(lineno)d",
}
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"
LOG_FILE = 'proxy_downloader.log'

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = LOG_FORMAT.copy()
        log_record["message"] = record.getMessage()
        log_record["level"] = record.levelname
        log_record["process"] = record.process
        log_record["time"] = self.formatTime(record, self.default_time_format)
        log_record["module"] = record.module
        log_record["funcName"] = record.funcName
        log_record["lineno"] = record.lineno
        if record.exc_info:
            log_record['exc_info'] = self.formatException(record.exc_info)
        return json.dumps(log_record, ensure_ascii=False)

formatter_file = JsonFormatter()
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter_console = logging.Formatter(CONSOLE_LOG_FORMAT)
console_handler.setFormatter(formatter_console)
logger.addHandler(console_handler)

USE_COLOR_LOGS = True

def colored_log(level: int, message: str, *args, **kwargs):
    RESET = '\033[0m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BOLD_RED = '\033[1m\033[91m'

    color = RESET
    if USE_COLOR_LOGS:
        if level == logging.INFO:
            color = GREEN
        elif level == logging.WARNING:
            color = YELLOW
        elif level == logging.ERROR:
            color = RED
        elif level == logging.CRITICAL:
            color = BOLD_RED
    else:
        color = RESET

    frame = inspect.currentframe().f_back
    pathname = frame.f_code.co_filename
    lineno = frame.f_lineno
    func = frame.f_code.co_name

    formatted_message = f"{color}{message}{RESET}" if USE_COLOR_LOGS else message

    record = logging.LogRecord(
        name=logger.name,
        level=level,
        pathname=pathname,
        lineno=lineno,
        msg=formatted_message,
        args=args,
        exc_info=kwargs.get('exc_info'),
        func=func,
        sinfo=None
    )
    logger.handle(record)

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è ---
class Protocols(Enum):
    """–ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤."""
    VLESS = "vless"
    TUIC = "tuic"
    HY2 = "hy2"
    SS = "ss"
    SSR = "ssr"
    TROJAN = "trojan"

@dataclass(frozen=True)
class ConfigFiles:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã."""
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt"

@dataclass(frozen=True)
class RetrySettings:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫."""
    MAX_RETRIES: int = 4
    RETRY_DELAY_BASE: int = 2

@dataclass(frozen=True)
class ConcurrencyLimits:
    """–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    MAX_CHANNELS: int = 60
    MAX_PROXIES_PER_CHANNEL: int = 50
    MAX_PROXIES_GLOBAL: int = 50

ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]
CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–º IPv4-–∞–¥—Ä–µ—Å–æ–º."""
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False

async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """–†–∞–∑—Ä–µ—à–∞–µ—Ç –∏–º—è —Ö–æ—Å—Ç–∞ –≤ IPv4-–∞–¥—Ä–µ—Å."""
    if is_valid_ipv4(hostname):
        return hostname

    try:
        async with asyncio.timeout(10):
            result = await resolver.query(hostname, 'A')
            resolved_ip = result[0].host
            if is_valid_ipv4(resolved_ip):
                return resolved_ip
            else:
                logger.debug(f"DNS resolved {hostname} to non-IPv4: {resolved_ip}")
                return None
    except asyncio.TimeoutError:
        logger.debug(f"DNS resolution timeout for {hostname}")
        return None
    except aiodns.error.DNSError as e:
        logger.debug(f"DNS resolution error for {hostname}: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error during DNS resolution for {hostname}: {e}", exc_info=True)
        return None

# --- –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ---

class ProfileName(Enum):
    """–ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–æ—Ñ–∏–ª–µ–π –ø—Ä–æ–∫—Å–∏."""
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    SSR = "SSR"
    TROJAN = "TROJAN"
    UNKNOWN = "Unknown Protocol"

class InvalidURLError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö URL-–∞–¥—Ä–µ—Å–æ–≤."""
    pass

class UnsupportedProtocolError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤."""
    pass

@dataclass(frozen=True, eq=True)
class ProxyParsedConfig:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏."""
    config_string: str
    protocol: str
    address: str
    port: int
    remark: str = ""
    query_params: Dict[str, str] = field(default_factory=dict)
    quality_score: int = 0

    def __hash__(self):
        """–•–µ—à–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
        return hash((self.config_string))

    def __str__(self):
        """–°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞."""
        return (f"ProxyConfig(protocol={self.protocol}, address={self.address}, "
                f"port={self.port}, config_string='{self.config_string[:50]}...', quality_score={self.quality_score}")

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        """–†–∞–∑–±–∏—Ä–∞–µ—Ç URL –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏."""
        protocol = next((p for p in ALLOWED_PROTOCOLS if config_string.startswith(p + "://")), None)
        if not protocol:
            try:
                decoded_config = base64.b64decode(config_string).decode('utf-8')
                protocol = next((p for p in ALLOWED_PROTOCOLS if decoded_config.startswith(p + "://")), None)
                if protocol:
                    config_string = decoded_config
                else:
                    logger.debug(f"Unsupported protocol after base64 decode: {config_string}")
                    return None
            except (ValueError, UnicodeDecodeError) as e:
                logger.debug(f"Base64 decode error for '{config_string}': {e}")
                return None

        try:
            parsed_url = urlparse(config_string)
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port:
                logger.debug(f"Could not extract address or port from URL: {config_string}")
                return None

            if not 1 <= port <= 65535:
                logger.debug(f"Invalid port number: {port} in URL: {config_string}")
                return None

            remark = parsed_url.fragment if parsed_url.fragment else ""
            query_params = {k: v[0] for k, v in parse_qs(parsed_url.query).items()} if parsed_url.query else {}

            return cls(
                config_string=config_string.split("#")[0],
                protocol=protocol,
                address=address,
                port=port,
                remark=remark,
                query_params=query_params,
            )

        except ValueError as e:
            logger.debug(f"URL parsing error for '{config_string}': {e}")
            return None

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---

QUALITY_SCORE_WEIGHTS = {
    "protocol": {"vless": 5, "trojan": 5, "tuic": 4, "hy2": 3, "ss": 2, "ssr": 1},
    "security": {"tls": 3, "none": 0},
    "transport": {"ws": 2, "websocket": 2, "grpc": 2, "tcp": 1, "udp": 0},
}

QUALITY_CATEGORIES = {
    "High": range(8, 15),  # Example ranges, adjust as needed
    "Medium": range(4, 8),
    "Low": range(0, 4),
}

def assess_proxy_quality(proxy_config: ProxyParsedConfig) -> int:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∫—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ—Å–∞."""
    score = 0
    protocol = proxy_config.protocol.lower()
    query_params = proxy_config.query_params

    # Protocol score
    score += QUALITY_SCORE_WEIGHTS["protocol"].get(protocol, 0)

    # Security score
    security = query_params.get("security", "none").lower()
    score += QUALITY_SCORE_WEIGHTS["security"].get(security, 0)

    # Transport score
    transport = query_params.get("transport", "tcp").lower()
    score += QUALITY_SCORE_WEIGHTS["transport"].get(transport, 0)

    return score

def get_quality_category(score: int) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–ª–ª–∞."""
    for category, score_range in QUALITY_CATEGORIES.items():
        if score in score_range:
            return category
    return "Unknown" # Fallback category

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> Tuple[List[str], str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–∫—Å–∏ –∏–∑ URL –∫–∞–Ω–∞–ª–∞."""
    headers = {'User-Agent': 'ProxyDownloader/1.0'}
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=15)

    while retries_attempted <= RETRY.MAX_RETRIES:
        try:
            async with session.get(channel_url, timeout=session_timeout, headers=headers) as response:
                response.raise_for_status()
                text = await response.text(encoding='utf-8', errors='ignore')

                if not text.strip():
                    colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
                    return [], "warning"

                try:
                    decoded_text = base64.b64decode(text.strip()).decode('utf-8')
                    return decoded_text.splitlines(), "success"
                except:
                    return text.splitlines(), "success"

        except aiohttp.ClientResponseError as e:
            colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª HTTP –æ—à–∏–±–∫—É {e.status}: {e.message}")
            return [], "warning"
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted)
            colored_log(logging.WARNING, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {channel_url} (–ø–æ–ø—ã—Ç–∫–∞ {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...")
            if retries_attempted == RETRY.MAX_RETRIES:
                colored_log(logging.ERROR, f"‚ùå –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({RETRY.MAX_RETRIES+1}) –¥–ª—è {channel_url}")
                return [], "error"
            await asyncio.sleep(retry_delay)
        retries_attempted += 1

    return [], "critical"

async def parse_and_filter_proxies(lines: List[str], resolver: aiodns.DNSResolver) -> List[ProxyParsedConfig]:
    """–†–∞–∑–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏."""
    parsed_configs: List[ProxyParsedConfig] = []
    processed_configs: Set[str] = set()

    for line in lines:
        line = line.strip()
        if not line:
            continue

        try:
            parsed_config = ProxyParsedConfig.from_url(line)
            if parsed_config is None:
                logger.debug(f"Skipping invalid proxy URL: {line}")
                continue

            resolved_ip = await resolve_address(parsed_config.address, resolver)

            if parsed_config.config_string in processed_configs:
                logger.debug(f"Skipping duplicate proxy: {parsed_config.config_string}")
                continue
            processed_configs.add(parsed_config.config_string)

            if resolved_ip:
                quality_score = assess_proxy_quality(parsed_config)
                parsed_config_with_score = dataclasses.replace(parsed_config, quality_score=quality_score)
                parsed_configs.append(parsed_config_with_score)

        except Exception as e:
            logger.error(f"Unexpected error parsing proxy URL '{line}': {e}", exc_info=True)
            continue

    return parsed_configs

def generate_proxy_profile_name(proxy_config: ProxyParsedConfig) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è –ø—Ä–æ–∫—Å–∏."""
    protocol = proxy_config.protocol.upper()
    type_ = proxy_config.query_params.get('type', 'unknown').lower()
    security = proxy_config.query_params.get('security', 'none').lower()

    if protocol == 'SS' and type_ == 'unknown':
        type_ = 'tcp'

    return f"{protocol}_{type_}_{security}"

def save_all_proxies_to_file(all_proxies: List[ProxyParsedConfig], output_file: str) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª."""
    total_proxies_count = 0
    unique_proxies: List[ProxyParsedConfig] = []
    seen_config_strings: Set[str] = set()

    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        for proxy_conf in all_proxies:
            if proxy_conf.config_string not in seen_config_strings:
                unique_proxies.append(proxy_conf)
                seen_config_strings.add(proxy_conf.config_string)

        with open(output_file, 'w', encoding='utf-8') as f:
            for proxy_conf in unique_proxies:
                profile_name = generate_proxy_profile_name(proxy_conf)
                quality_category = get_quality_category(proxy_conf.quality_score) # Get quality category
                config_line = f"{proxy_conf.config_string}#PROFILE={profile_name};QUALITY_SCORE={proxy_conf.quality_score};QUALITY_CATEGORY={quality_category}" # Include category
                f.write(config_line + "\n")
                total_proxies_count += 1

    except Exception as e:
        logger.error(f"Error saving proxies to file '{output_file}': {e}", exc_info=True)
    return total_proxies_count

async def load_channel_urls(all_urls_file: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç URL –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    channel_urls: List[str] = []
    try:
        with open(all_urls_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url:
                    channel_urls.append(url)
    except FileNotFoundError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è –§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª.")
        try:
            open(all_urls_file, 'w').close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞ {all_urls_file}: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"Error opening/reading file {all_urls_file}: {e}", exc_info=True)
    return channel_urls

async def process_channel_task(channel_url: str, session: aiohttp.ClientSession, resolver: aiodns.DNSResolver, protocol_counts: defaultdict[str, int]) -> Tuple[int, str, List[ProxyParsedConfig]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω URL –∫–∞–Ω–∞–ª–∞."""
    colored_log(logging.INFO, f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: {channel_url}")
    lines, status = await download_proxies_from_channel(channel_url, session)
    if status == "success":
        parsed_proxies = await parse_and_filter_proxies(lines, resolver)
        channel_proxies_count_channel = len(parsed_proxies)
        for proxy in parsed_proxies:
            protocol_counts[proxy.protocol] += 1
        colored_log(logging.INFO, f"‚úÖ –ö–∞–Ω–∞–ª {channel_url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ù–∞–π–¥–µ–Ω–æ {channel_proxies_count_channel} –ø—Ä–æ–∫—Å–∏.")
        return channel_proxies_count_channel, status, parsed_proxies
    else:
        colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {status}.")
        return 0, status, []

async def load_and_process_channels(channel_urls: List[str], session: aiohttp.ClientSession, resolver: aiodns.DNSResolver) -> Tuple[int, int, defaultdict[str, int], List[ProxyParsedConfig], defaultdict[str, int]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ URL –∫–∞–Ω–∞–ª—ã."""
    channels_processed_successfully = 0
    total_proxies_downloaded = 0
    protocol_counts: defaultdict[str, int] = defaultdict(int)
    channel_status_counts: defaultdict[str, int] = defaultdict(int)
    all_proxies: List[ProxyParsedConfig] = []

    channel_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_CHANNELS)
    channel_tasks = []

    for channel_url in channel_urls:
        async def task_wrapper(url):
            async with channel_semaphore:
                return await process_channel_task(url, session, resolver, protocol_counts)

        task = asyncio.create_task(task_wrapper(channel_url))
        channel_tasks.append(task)

    channel_results = await asyncio.gather(*channel_tasks)

    for proxies_count, status, proxies_list in channel_results:
        total_proxies_downloaded += proxies_count
        if status == "success":
            channels_processed_successfully += 1
        channel_status_counts[status] += 1
        all_proxies.extend(proxies_list)

    return total_proxies_downloaded, channels_processed_successfully, protocol_counts, all_proxies, channel_status_counts

def output_statistics(start_time: float, total_channels: int, channels_processed_successfully: int, channel_status_counts: defaultdict[str, int], total_proxies_downloaded: int, all_proxies_saved_count: int, protocol_counts: defaultdict[str, int], output_file: str, all_proxies: List[ProxyParsedConfig]):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–∫—Å–∏."""
    end_time = time.time()
    elapsed_time = end_time - start_time

    colored_log(logging.INFO, "==================== üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–û–ö–°–ò ====================")
    colored_log(logging.INFO, f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {elapsed_time:.2f} —Å–µ–∫")
    colored_log(logging.INFO, f"üîó –í—Å–µ–≥–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_channels}")
    colored_log(logging.INFO, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {channels_processed_successfully}/{total_channels}")

    colored_log(logging.INFO, "\nüìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
    for status_key in ["success", "warning", "error", "critical"]:
        count = channel_status_counts.get(status_key, 0)
        if count > 0:
            status_text, color = "", ""
            if status_key == "success":
                status_text, color = "–£–°–ü–ï–®–ù–û", '\033[92m'
            elif status_key == "warning":
                status_text, color = "–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï", '\033[93m'
            elif status_key in ["error", "critical"]:
                status_text, color = "–û–®–ò–ë–ö–ê", '\033[91m'
            else:
                status_text, color = status_key.upper(), '\033[0m'

            colored_log(logging.INFO, f"  - {status_text}: {count} –∫–∞–Ω–∞–ª–æ–≤")

    colored_log(logging.INFO, f"\n‚ú® –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {total_proxies_downloaded}")
    colored_log(logging.INFO, f"üìù –í—Å–µ–≥–æ –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ, –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {all_proxies_saved_count} (–≤ {output_file})")

    colored_log(logging.INFO, "\nüî¨ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º (–Ω–∞–π–¥–µ–Ω–æ):")
    if protocol_counts:
        for protocol, count in protocol_counts.items():
            colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
    else:
        colored_log(logging.INFO, "   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.")

    # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞ ---
    quality_category_counts = defaultdict(int)
    for proxy in all_proxies:
        quality_category = get_quality_category(proxy.quality_score)
        quality_category_counts[quality_category] += 1

    colored_log(logging.INFO, "\n‚≠êÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–∫—Å–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞:")
    if quality_category_counts:
        for category, count in quality_category_counts.items():
            colored_log(logging.INFO, f"   - {category}: {count} –ø—Ä–æ–∫—Å–∏")
    else:
        colored_log(logging.INFO, "   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞.")


    colored_log(logging.INFO, "======================== üèÅ –ö–û–ù–ï–¶ –°–¢–ê–¢–ò–°–¢–ò–ö–ò =========================")

async def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(description="Proxy Downloader Script")
    parser.add_argument('--nocolorlogs', action='store_true', help='Disable colored console logs')
    args = parser.parse_args()

    global USE_COLOR_LOGS
    if args.nocolorlogs:
        USE_COLOR_LOGS = False

    try:
        start_time = time.time()
        channel_urls = await load_channel_urls(CONFIG_FILES.ALL_URLS)
        if not channel_urls:
            colored_log(logging.WARNING, "–ù–µ—Ç URL-–∞–¥—Ä–µ—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())
        async with aiohttp.ClientSession() as session:
            total_proxies_downloaded, channels_processed_successfully, protocol_counts, all_proxies, channel_status_counts = await load_and_process_channels(channel_urls, session, resolver)

        all_proxies_saved_count = save_all_proxies_to_file(all_proxies, CONFIG_FILES.OUTPUT_ALL_CONFIG)

        output_statistics(start_time, len(channel_urls), channels_processed_successfully, channel_status_counts, total_proxies_downloaded, all_proxies_saved_count, protocol_counts, CONFIG_FILES.OUTPUT_ALL_CONFIG, all_proxies)

    except Exception as e:
        logger.critical(f"Unexpected error in main(): {e}", exc_info=True)
        sys.exit(1)
    finally:
        colored_log(logging.INFO, "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    asyncio.run(main())
