import asyncio
import aiodns
import re
import os
import logging
import ipaddress
import io
import uuid
import string
import base64
import aiohttp
import time
import json
import functools
import inspect
import sys
import argparse  # –î–æ–±–∞–≤–∏–ª–∏ –∏–º–ø–æ—Ä—Ç argparse

from enum import Enum
from urllib.parse import urlparse, parse_qs, urlsplit
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
LOG_FORMAT = {
    "time": "%(asctime)s",
    "level": "%(levelname)s",
    "message": "%(message)s",
    "process": "%(process)s",
    "module": "%(module)s",
    "funcName": "%(funcName)s",
    "lineno": "%(lineno)d",
}
CONSOLE_LOG_FORMAT = "[%(levelname)s] %(message)s"  # –§–æ—Ä–º–∞—Ç –¥–ª—è –∫–æ–Ω—Å–æ–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
LOG_FILE = 'proxy_downloader.log'  # –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–∞ (—É—Ä–æ–≤–µ–Ω—å WARNING –∏ –≤—ã—à–µ, —Ñ–æ—Ä–º–∞—Ç JSON)
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.WARNING)


class JsonFormatter(logging.Formatter):
    """–§–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤ –≤ JSON."""

    def format(self, record):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∑–∞–ø–∏—Å—å –ª–æ–≥–∞ –≤ JSON."""
        log_record = LOG_FORMAT.copy()
        log_record["message"] = record.getMessage()
        log_record["level"] = record.levelname
        log_record["process"] = record.process
        log_record["time"] = self.formatTime(record, self.default_time_format)
        log_record["module"] = record.module
        log_record["funcName"] = record.funcName
        log_record["lineno"] = record.lineno
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π, –µ—Å–ª–∏ –µ—Å—Ç—å
        if record.exc_info:
            log_record['exc_info'] = self.formatException(record.exc_info)
        return json.dumps(log_record, ensure_ascii=False)

formatter_file = JsonFormatter()
file_handler.setFormatter(formatter_file)
logger.addHandler(file_handler)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–Ω—Å–æ–ª–∏ (—É—Ä–æ–≤–µ–Ω—å INFO –∏ –≤—ã—à–µ, —Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter_console = logging.Formatter(CONSOLE_LOG_FORMAT)
console_handler.setFormatter(formatter_console)
logger.addHandler(console_handler)


USE_COLOR_LOGS = True  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Ü–≤–µ—Ç–Ω—ã—Ö –ª–æ–≥–æ–≤ (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ config)

def colored_log(level: int, message: str, *args, **kwargs):
    """–í—ã–≤–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ü–≤–µ—Ç–æ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
       –¶–≤–µ—Ç–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ USE_COLOR_LOGS.
    """
    RESET = '\033[0m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BOLD_RED = '\033[1m\033[91m'

    color = RESET
    if USE_COLOR_LOGS:
        if level == logging.INFO:
            color = GREEN
        elif level == logging.WARNING:
            color = YELLOW
        elif level == logging.ERROR:
            color = RED
        elif level == logging.CRITICAL:
            color = BOLD_RED
    else:
        color = RESET  # No color if USE_COLOR_LOGS is False

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–∑—ã–≤–∞—é—â–µ–π —Å—Ç–æ—Ä–æ–Ω–µ.  –§—Ä–µ–π–º —Å—Ç–µ–∫–∞ 1 - —ç—Ç–æ –≤—ã–∑—ã–≤–∞—é—â–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ colored_log.
    frame = inspect.currentframe().f_back
    pathname = frame.f_code.co_filename
    lineno = frame.f_lineno
    func = frame.f_code.co_name

    formatted_message = f"{color}{message}{RESET}" if USE_COLOR_LOGS else message  # Conditional coloring

    record = logging.LogRecord(
        name=logger.name,
        level=level,
        pathname=pathname,
        lineno=lineno,
        msg=formatted_message,
        args=args,
        exc_info=kwargs.get('exc_info'),
        func=func,
        sinfo=None
    )
    logger.handle(record)


# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è ---
class Protocols(Enum):
    """–ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤."""
    VLESS = "vless"
    TUIC = "tuic"
    HY2 = "hy2"
    SS = "ss"
    SSR = "ssr"
    TROJAN = "trojan"


@dataclass(frozen=True)
class ConfigFiles:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã."""
    ALL_URLS: str = "channel_urls.txt"
    OUTPUT_ALL_CONFIG: str = "configs/proxy_configs_all.txt"


@dataclass(frozen=True)
class RetrySettings:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫."""
    MAX_RETRIES: int = 4
    RETRY_DELAY_BASE: int = 2


@dataclass(frozen=True)
class ConcurrencyLimits:
    """–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    MAX_CHANNELS: int = 60
    MAX_PROXIES_PER_CHANNEL: int = 50
    MAX_PROXIES_GLOBAL: int = 50


ALLOWED_PROTOCOLS = [proto.value for proto in Protocols]
CONFIG_FILES = ConfigFiles()
RETRY = RetrySettings()
CONCURRENCY = ConcurrencyLimits()

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

@functools.lru_cache(maxsize=1024)
def is_valid_ipv4(hostname: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–º IPv4-–∞–¥—Ä–µ—Å–æ–º.

    Args:
        hostname: –°—Ç—Ä–æ–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.

    Returns:
        True, –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –¥–æ–ø—É—Å—Ç–∏–º—ã–º IPv4-–∞–¥—Ä–µ—Å–æ–º, –∏–Ω–∞—á–µ False.
    """
    try:
        ipaddress.IPv4Address(hostname)
        return True
    except ipaddress.AddressValueError:
        return False


async def resolve_address(hostname: str, resolver: aiodns.DNSResolver) -> Optional[str]:
    """
    –†–∞–∑—Ä–µ—à–∞–µ—Ç –∏–º—è —Ö–æ—Å—Ç–∞ –≤ IPv4-–∞–¥—Ä–µ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π DNS-—Ä–µ–∑–æ–ª–≤–µ—Ä.

    Args:
        hostname: –ò–º—è —Ö–æ—Å—Ç–∞ –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è.
        resolver: –≠–∫–∑–µ–º–ø–ª—è—Ä aiodns.DNSResolver –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è DNS-–∑–∞–ø—Ä–æ—Å–æ–≤.

    Returns:
        –°—Ç—Ä–æ–∫–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è IPv4-–∞–¥—Ä–µ—Å, –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ None.
    """
    if is_valid_ipv4(hostname):
        return hostname  # –£–∂–µ IP-–∞–¥—Ä–µ—Å

    try:
        async with asyncio.timeout(10):  # –¢–∞–π–º–∞—É—Ç DNS —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            result = await resolver.query(hostname, 'A')
            resolved_ip = result[0].host
            if is_valid_ipv4(resolved_ip):
                return resolved_ip
            else:
                logger.debug(f"DNS resolved {hostname} to non-IPv4: {resolved_ip}") # Debug level
                return None
    except asyncio.TimeoutError:
        logger.debug(f"DNS resolution timeout for {hostname}") # Debug level
        return None
    except aiodns.error.DNSError as e:
        logger.debug(f"DNS resolution error for {hostname}: {e}") # Debug level
        return None
    except Exception as e:
        logger.error(f"Unexpected error during DNS resolution for {hostname}: {e}", exc_info=True)
        return None


# --- –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ---

class ProfileName(Enum):
    """–ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–æ—Ñ–∏–ª–µ–π –ø—Ä–æ–∫—Å–∏."""
    VLESS = "VLESS"
    TUIC = "TUIC"
    HY2 = "HY2"
    SS = "SS"
    SSR = "SSR"
    TROJAN = "TROJAN"
    UNKNOWN = "Unknown Protocol"


class InvalidURLError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º–æ–µ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ URL-–∞–¥—Ä–µ—Å–∞."""
    pass


class UnsupportedProtocolError(ValueError):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º–æ–µ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞."""
    pass


@dataclass(frozen=True, eq=True) # –î–æ–±–∞–≤–∏–ª–∏ eq=True –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ —Å–ø–∏—Å–∫–∞—Ö/–º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö
class ProxyParsedConfig:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏."""
    config_string: str
    protocol: str
    address: str
    port: int
    remark: str = ""
    query_params: Dict[str, str] = field(default_factory=dict)

    def __hash__(self):
        """–•–µ—à–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏ (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)."""
        return hash((self.config_string)) # –•–µ—à–∏—Ä—É–µ–º –ø–æ config_string –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

    def __str__(self):
        """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω–æ–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞."""
        return (f"ProxyConfig(protocol={self.protocol}, address={self.address}, "
                f"port={self.port}, config_string='{self.config_string[:50]}...')")

    @classmethod
    def from_url(cls, config_string: str) -> Optional["ProxyParsedConfig"]:
        """
        –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ (URL) –≤ –æ–±—ä–µ–∫—Ç ProxyParsedConfig.

        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç base64-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç—Ä–æ–∫, –Ω–µ –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤.

        Args:
            config_string: –°—Ç—Ä–æ–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ (URL).

        Returns:
            –û–±—ä–µ–∫—Ç ProxyParsedConfig, –µ—Å–ª–∏ —Ä–∞–∑–±–æ—Ä —É—Å–ø–µ—à–µ–Ω, –∏–Ω–∞—á–µ None.

        Raises:
            ValueError: –ï—Å–ª–∏ URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–æ–±—Ä–∞–Ω –∏–ª–∏ –ø–æ—Ä—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º.
        """
        protocol = next((p for p in ALLOWED_PROTOCOLS if config_string.startswith(p + "://")), None)
        if not protocol:
            # –ü–æ–ø—ã—Ç–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å base64, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π URL
            try:
                decoded_config = base64.b64decode(config_string).decode('utf-8')
                protocol = next((p for p in ALLOWED_PROTOCOLS if decoded_config.startswith(p + "://")), None)
                if protocol:
                    config_string = decoded_config # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
                else:
                    logger.debug(f"Unsupported protocol after base64 decode: {config_string}") # Debug level
                    return None
            except (ValueError, UnicodeDecodeError) as e: # –õ–æ–≤–∏–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è base64
                logger.debug(f"Base64 decode error for '{config_string}': {e}") # Debug level
                return None

        try:
            parsed_url = urlparse(config_string)
            address = parsed_url.hostname
            port = parsed_url.port
            if not address or not port:
                logger.debug(f"Could not extract address or port from URL: {config_string}") # Debug level
                return None

            if not 1 <= port <= 65535:  # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Ä—Ç–∞
                logger.debug(f"Invalid port number: {port} in URL: {config_string}") # Debug level
                return None

            remark = parsed_url.fragment if parsed_url.fragment else ""
            query_params = {k: v[0] for k, v in parse_qs(parsed_url.query).items()} if parsed_url.query else {}

            return cls(
                config_string=config_string.split("#")[0], # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –∏–∑ config_string
                protocol=protocol,
                address=address,
                port=port,
                remark=remark,
                query_params=query_params,
            )

        except ValueError as e:
            logger.debug(f"URL parsing error for '{config_string}': {e}") # Debug level
            return None


# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---

async def download_proxies_from_channel(channel_url: str, session: aiohttp.ClientSession) -> Tuple[List[str], str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–∞.

    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö —Å–µ—Ç–∏ –∏–ª–∏ HTTP, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç base64-–∫–æ–Ω—Ç–µ–Ω—Ç.

    Args:
        channel_url: URL-–∞–¥—Ä–µ—Å –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∫—Å–∏.
        session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è aiohttp.ClientSession.

    Returns:
        –ö–æ—Ä—Ç–µ–∂: (—Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏, —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç—É—Å–∞).
        –°—Ç–∞—Ç—É—Å –º–æ–∂–µ—Ç –±—ã—Ç—å: "success", "warning", "error", "critical".
    """
    headers = {'User-Agent': 'ProxyDownloader/1.0'}
    retries_attempted = 0
    session_timeout = aiohttp.ClientTimeout(total=15)

    while retries_attempted <= RETRY.MAX_RETRIES:
        try:
            async with session.get(channel_url, timeout=session_timeout, headers=headers) as response:
                response.raise_for_status()
                text = await response.text(encoding='utf-8', errors='ignore')

                if not text.strip():
                    colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
                    return [], "warning"

                try:
                    decoded_text = base64.b64decode(text.strip()).decode('utf-8')
                    return decoded_text.splitlines(), "success"
                except:
                    return text.splitlines(), "success"

        except aiohttp.ClientResponseError as e:
            colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –≤–µ—Ä–Ω—É–ª HTTP –æ—à–∏–±–∫—É {e.status}: {e.message}")
            return [], "warning"
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            retry_delay = RETRY.RETRY_DELAY_BASE * (2 ** retries_attempted)
            colored_log(logging.WARNING, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {channel_url} (–ø–æ–ø—ã—Ç–∫–∞ {retries_attempted+1}/{RETRY.MAX_RETRIES+1}): {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...")
            if retries_attempted == RETRY.MAX_RETRIES:
                colored_log(logging.ERROR, f"‚ùå –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({RETRY.MAX_RETRIES+1}) –¥–ª—è {channel_url}")
                return [], "error"
            await asyncio.sleep(retry_delay)
        retries_attempted += 1

    return [], "critical"


async def parse_and_filter_proxies(lines: List[str], resolver: aiodns.DNSResolver) -> List[ProxyParsedConfig]:
    """
    –†–∞–∑–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫.

    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–º–µ–Ω —Ö–æ—Å—Ç–æ–≤ –≤ IP-–∞–¥—Ä–µ—Å–∞, —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–≤–µ—Ä–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Args:
        lines: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏.
        resolver: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π DNS-—Ä–µ–∑–æ–ª–≤–µ—Ä aiodns.DNSResolver.

    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ ProxyParsedConfig –ø–æ—Å–ª–µ —Ä–∞–∑–±–æ—Ä–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    """
    parsed_configs: List[ProxyParsedConfig] = [] # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞
    processed_configs: Set[str] = set() # Set –¥–ª—è config_string

    for line in lines:
        line = line.strip()
        if not line:
            continue

        try:
            parsed_config = ProxyParsedConfig.from_url(line)
            if parsed_config is None:
                logger.debug(f"Skipping invalid proxy URL: {line}") # Debug level
                continue

            resolved_ip = await resolve_address(parsed_config.address, resolver)

            if parsed_config.config_string in processed_configs:
                logger.debug(f"Skipping duplicate proxy: {parsed_config.config_string}") # Debug level
                continue
            processed_configs.add(parsed_config.config_string)

            if resolved_ip:
                parsed_configs.append(parsed_config)

        except Exception as e: #  –õ–æ–≤–∏–º –±–æ–ª–µ–µ –æ–±—â–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ—Ä–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–æ–∫
            logger.error(f"Unexpected error parsing proxy URL '{line}': {e}", exc_info=True) # –õ–æ–≥–∏—Ä—É–µ–º unexpected errors
            continue # Continue to next line

    return parsed_configs


def generate_proxy_profile_name(proxy_config: ProxyParsedConfig) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è –ø—Ä–æ–∫—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞.

    Args:
        proxy_config: –û–±—ä–µ–∫—Ç ProxyParsedConfig.

    Returns:
        –°—Ç—Ä–æ–∫–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –∏–º—è –ø—Ä–æ—Ñ–∏–ª—è –ø—Ä–æ–∫—Å–∏.
    """
    protocol = proxy_config.protocol.upper()
    type_ = proxy_config.query_params.get('type', 'unknown').lower()
    security = proxy_config.query_params.get('security', 'none').lower()

    if protocol == 'SS' and type_ == 'unknown':
        type_ = 'tcp'

    return f"{protocol}_{type_}_{security}"


def save_all_proxies_to_file(all_proxies: List[ProxyParsedConfig], output_file: str) -> int:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª, —É–¥–∞–ª—è—è –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º.

    Args:
        all_proxies: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ ProxyParsedConfig –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
        output_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏.

    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏.
    """
    total_proxies_count = 0
    unique_proxies: List[ProxyParsedConfig] = [] # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞
    seen_config_strings: Set[str] = set()

    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        for proxy_conf in all_proxies:
            if proxy_conf.config_string not in seen_config_strings:
                unique_proxies.append(proxy_conf)
                seen_config_strings.add(proxy_conf.config_string)

        with open(output_file, 'w', encoding='utf-8') as f:
            for proxy_conf in unique_proxies:
                profile_name = generate_proxy_profile_name(proxy_conf)
                config_line = f"{proxy_conf.config_string}#{profile_name}"
                f.write(config_line + "\n")
                total_proxies_count += 1

    except Exception as e:
        logger.error(f"Error saving proxies to file '{output_file}': {e}", exc_info=True) # Added filename to log
    return total_proxies_count


async def load_channel_urls(all_urls_file: str) -> List[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞.

    –°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.

    Args:
        all_urls_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º—É URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤.

    Returns:
        –°–ø–∏—Å–æ–∫ URL-–∞–¥—Ä–µ—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞.
    """
    channel_urls: List[str] = [] # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞
    try:
        with open(all_urls_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url:
                    channel_urls.append(url)
    except FileNotFoundError:
        colored_log(logging.WARNING, f"‚ö†Ô∏è –§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª.")
        try:  # –î–æ–±–∞–≤–ª–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            open(all_urls_file, 'w').close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞ {all_urls_file}: {e}", exc_info=True) # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    except Exception as e:
        logger.error(f"Error opening/reading file {all_urls_file}: {e}", exc_info=True)
    return channel_urls


async def process_channel_task(channel_url: str, session: aiohttp.ClientSession, resolver: aiodns.DNSResolver, protocol_counts: defaultdict[str, int]) -> Tuple[int, str, List[ProxyParsedConfig]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω URL-–∞–¥—Ä–µ—Å –∫–∞–Ω–∞–ª–∞: –∑–∞–≥—Ä—É–∂–∞–µ—Ç, —Ä–∞–∑–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–æ–∫—Å–∏.

    Args:
        channel_url: URL-–∞–¥—Ä–µ—Å –∫–∞–Ω–∞–ª–∞.
        session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è aiohttp.ClientSession.
        resolver: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π DNS-—Ä–µ–∑–æ–ª–≤–µ—Ä aiodns.DNSResolver.
        protocol_counts: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤.

    Returns:
        –ö–æ—Ä—Ç–µ–∂: (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏, —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Å–ø–∏—Å–æ–∫ ProxyParsedConfig).
    """
    colored_log(logging.INFO, f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: {channel_url}")
    lines, status = await download_proxies_from_channel(channel_url, session)
    if status == "success":
        parsed_proxies = await parse_and_filter_proxies(lines, resolver)
        channel_proxies_count_channel = len(parsed_proxies)
        for proxy in parsed_proxies:
            protocol_counts[proxy.protocol] += 1
        colored_log(logging.INFO, f"‚úÖ –ö–∞–Ω–∞–ª {channel_url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ù–∞–π–¥–µ–Ω–æ {channel_proxies_count_channel} –ø—Ä–æ–∫—Å–∏.")
        return channel_proxies_count_channel, status, parsed_proxies
    else:
        colored_log(logging.WARNING, f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel_url} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {status}.")
        return 0, status, []


async def load_and_process_channels(channel_urls: List[str], session: aiohttp.ClientSession, resolver: aiodns.DNSResolver) -> Tuple[int, int, defaultdict[str, int], List[ProxyParsedConfig], defaultdict[str, int]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º.

    Args:
        channel_urls: –°–ø–∏—Å–æ–∫ URL-–∞–¥—Ä–µ—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤.
        session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è aiohttp.ClientSession.
        resolver: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π DNS-—Ä–µ–∑–æ–ª–≤–µ—Ä aiodns.DNSResolver.

    Returns:
        –ö–æ—Ä—Ç–µ–∂: (–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤,
                 —Å—á–µ—Ç—á–∏–∫ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤, —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏, —Å—á–µ—Ç—á–∏–∫ —Å—Ç–∞—Ç—É—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤).
    """
    channels_processed_successfully = 0
    total_proxies_downloaded = 0
    protocol_counts: defaultdict[str, int] = defaultdict(int)
    channel_status_counts: defaultdict[str, int] = defaultdict(int)
    all_proxies: List[ProxyParsedConfig] = []

    channel_semaphore = asyncio.Semaphore(CONCURRENCY.MAX_CHANNELS)
    channel_tasks = []

    for channel_url in channel_urls:
        async def task_wrapper(url): # Wrapper function to manage semaphore and handle exceptions in tasks
            async with channel_semaphore:
                return await process_channel_task(url, session, resolver, protocol_counts) # Pass protocol_counts

        task = asyncio.create_task(task_wrapper(channel_url))
        channel_tasks.append(task)

    channel_results = await asyncio.gather(*channel_tasks) # Await all tasks

    for proxies_count, status, proxies_list in channel_results: # Process results from each channel
        total_proxies_downloaded += proxies_count
        if status == "success":
            channels_processed_successfully += 1
        channel_status_counts[status] += 1 # Count channel statuses
        all_proxies.extend(proxies_list) # Extend list of all proxies

    return total_proxies_downloaded, channels_processed_successfully, protocol_counts, all_proxies, channel_status_counts


def output_statistics(start_time: float, total_channels: int, channels_processed_successfully: int, channel_status_counts: defaultdict[str, int], total_proxies_downloaded: int, all_proxies_saved_count: int, protocol_counts: defaultdict[str, int], output_file: str):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–∫—Å–∏.

    Args:
        start_time: –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞.
        total_channels: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        channels_processed_successfully: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.
        channel_status_counts: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç—É—Å–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤.
        total_proxies_downloaded: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø—Ä–æ–∫—Å–∏.
        all_proxies_saved_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∫—Å–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤).
        protocol_counts: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–∫—Å–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.
        output_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∫—É–¥–∞ –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–æ–∫—Å–∏.
    """
    end_time = time.time()
    elapsed_time = end_time - start_time

    colored_log(logging.INFO, "==================== üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–û–ö–°–ò ====================")
    colored_log(logging.INFO, f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {elapsed_time:.2f} —Å–µ–∫")
    colored_log(logging.INFO, f"üîó –í—Å–µ–≥–æ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_channels}")
    colored_log(logging.INFO, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {channels_processed_successfully}/{total_channels}")

    colored_log(logging.INFO, "\nüìä –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
    for status_key in ["success", "warning", "error", "critical"]:
        count = channel_status_counts.get(status_key, 0)
        if count > 0:
            status_text, color = "", "" # Initialize to avoid unbound variable error
            if status_key == "success":
                status_text, color = "–£–°–ü–ï–®–ù–û", '\033[92m'
            elif status_key == "warning":
                status_text, color = "–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï", '\033[93m'
            elif status_key in ["error", "critical"]:
                status_text, color = "–û–®–ò–ë–ö–ê", '\033[91m'
            else:
                status_text, color = status_key.upper(), '\033[0m'

            colored_log(logging.INFO, f"  - {status_text}: {count} –∫–∞–Ω–∞–ª–æ–≤")

    colored_log(logging.INFO, f"\n‚ú® –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {total_proxies_downloaded}")
    colored_log(logging.INFO, f"üìù –í—Å–µ–≥–æ –ø—Ä–æ–∫—Å–∏ (–≤—Å–µ, –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {all_proxies_saved_count} (–≤ {output_file})")

    colored_log(logging.INFO, "\nüî¨ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º (–Ω–∞–π–¥–µ–Ω–æ):")
    if protocol_counts:
        for protocol, count in protocol_counts.items():
            colored_log(logging.INFO, f"   - {protocol.upper()}: {count}")
    else:
        colored_log(logging.INFO, "   –ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º.")

    colored_log(logging.INFO, "======================== üèÅ –ö–û–ù–ï–¶ –°–¢–ê–¢–ò–°–¢–ò–ö–ò =========================")


async def main() -> None:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–∫—Å–∏.

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç URL-–∞–¥—Ä–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    """
    parser = argparse.ArgumentParser(description="Proxy Downloader Script") # Create argument parser
    parser.add_argument('--nocolorlogs', action='store_true', help='Disable colored console logs') # Add --nocolorlogs flag
    args = parser.parse_args() # Parse arguments

    global USE_COLOR_LOGS # Access global flag
    if args.nocolorlogs: # If flag is set
        USE_COLOR_LOGS = False # Disable colored logs

    try:
        start_time = time.time()
        channel_urls = await load_channel_urls(CONFIG_FILES.ALL_URLS)
        if not channel_urls:
            colored_log(logging.WARNING, "–ù–µ—Ç URL-–∞–¥—Ä–µ—Å–æ–≤ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return  # Exit if no URLs

        resolver = aiodns.DNSResolver(loop=asyncio.get_event_loop())
        async with aiohttp.ClientSession() as session:
            total_proxies_downloaded, channels_processed_successfully, protocol_counts, all_proxies, channel_status_counts = await load_and_process_channels(channel_urls, session, resolver)

        all_proxies_saved_count = save_all_proxies_to_file(all_proxies, CONFIG_FILES.OUTPUT_ALL_CONFIG)

        output_statistics(start_time, len(channel_urls), channels_processed_successfully, channel_status_counts, total_proxies_downloaded, all_proxies_saved_count, protocol_counts, CONFIG_FILES.OUTPUT_ALL_CONFIG) # Pass output file

    except Exception as e:
        logger.critical(f"Unexpected error in main(): {e}", exc_info=True)
        sys.exit(1) # Exit with error code on critical error
    finally:
        colored_log(logging.INFO, "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∫—Å–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    asyncio.run(main())
